%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Descr:       Vorlage für Berichte der DHBW-Karlsruhe, BibTeX
%% Author:      Prof. Dr. Jürgen Vollmer, vollmer AT dhbw-karlsruhe.de
%% $Id: bericht.bib,v 1.10 2016/03/16 12:27:42 vollmer draft $
%% -*- coding: utf-8 -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%
%%  declararion of abbreviations
%%%%
@STRING{addison		= "Addison-Wesley"}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@BOOK{knuth.1984a,
AUTHOR		= "Knuth, Donald E.",
TITLE		= "The \TeX{}book",
PUBLISHER	= addison,
YEAR		= 1984
}

@BOOK{lamport.1995a,
AUTHOR		= "Lamport, Leslie",
TITLE		= "Das \LaTeX\ Handbuch",
PUBLISHER	= addison,
YEAR		= 1995
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Referenzieren von URL's
@MISC{dante.2010a,
  AUTHOR	= "Dante",
  TITLE		= "Webseite der \emph{Deutschsprachige Anwendervereinigung TeX e.V.}",
  HOWPUBLISHED 	= "\url{http://www.dante.de}",
  YEAR		= 2010,
  MONTH		= jan
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%online 
@online{coex_clover,
  author  = {Coex},
  title   = {COEX Clover},
  urldate = {2023-02-06},
  url     = {https://clover.coex.tech/en/}
}

@online{coex_clover_git,
  author = {Coex},
  title = {COEX Clover Github},
  urldate= {2023-05-21},
  url = {https://github.com/CopterExpress/clover/}
  }

@online{px4,
  author  = {PX4 Autopilot},
  title   = {Basic concepts},
  urldate = {2023-04-01},
  date    = {2022-08-15},
  url     = {https://docs.px4.io/main/en/getting_started/px4_basic_concepts.html}
}

@online{ROSconcepts,
  author  = {Open Robotics},
  title   = {ROS concepts},
  urldate = {2023-04-15},
  date    = {2022-09-20},
  url     = {http://wiki.ros.org/ROS/Concepts}
}

@online{ROSIntroduction,
  author  = {Open Robotics},
  title   = {ROS/Introduction},
  urldate = {2023-04-17},
  date    = {2018-08-08},
  url     = {http://wiki.ros.org/ROS/Introduction}
}

@online{ROSNodes,
  author  = {Open Robotics},
  title   = {Nodes},
  urldate = {2023-04-20},
  date    = {2018-12-04},
  url     = {http://wiki.ros.org/Nodes}
}

@online{ROSTopics,
  author  = {Open Robotics},
  title   = {Topics},
  urldate = {2023-04-20},
  date    = {2019-02-20},
  url     = {https://wiki.ros.org/Topics}
}

@online{ROSMessages,
  author  = {Open Robotics},
  title   = {Messsages},
  urldate = {2023-04-20},
  date    = {2016-08-26},
  url     = {https://wiki.ros.org/Messages}
}

@online{Lasertriangulation,
  author  = {SmartRay},
  title   = {Laser Triangulation},
  urldate = {2023-04-20},
  url     = {https://www.smartray.com/de/glossar/laser-triangulation/}
}

@online{strukturiertsLicht,
  author  = {Matthew McMillion},
  title   = {Wie funktioniert das 3D-Scannen mit strukturiertem Licht?},
  urldate = {2023-04-20},
  date    = {2022-11-25},
  url     = {https://www.artec3d.com/de/learning-center/structured-light-3d-scanning#:~:text=Wenn%20das%20strukturierte%20Licht%20auf,Linien%20oder%20Streifen%20strukturiert%20wird.}
}

@online{TOF,
  title   = {Time of Flight Sensor - ein Durchbruch beim 3D-Scannen?},
  urldate = {2023-04-20},
  date    = {2020-03-06},
  url     = {https://www.alza.de/time-of-flight-sensor}
}

@online{TiefenKamera,
  author  = {Alain Bruno Kamwa},
  title   = {Time-of-Flight- (ToF) -Verfahren mit Lidar-Systemen},
  urldate = {2023-04-20},
  date    = {2022-01-31},
  url     = {https://www.all-electronics.de/elektronik-entwicklung/time-of-flight-und-40tof-und-41-verfahren-mit-lidar-systemen-632.html#:~:text=Das%20grundlegende%20Prinzip%20ist%20einfach,gemessenen%20Laufzeit%20des%20Lichts%20errechnet.}
}

@online{filesystem,
  author  = {Open Robotics},
  title   = {ROS/Concepts},
  urldate = {2023-05-19},
  date    = {2022-09-20},
  url     = {http://wiki.ros.org/ROS/Concepts#Filesystem_Concepts}
}

@online{computationGraph,
  author  = {Open Robotics},
  title   = {ROS/Concepts},
  urldate = {2023-05-19},
  date    = {2022-09-20},
  url     = {http://wiki.ros.org/ROS/Concepts#Computation_Graph}
}

@online{roslaunch,
  author  = {Open Robotics},
  title   = {roslaunch},
  urldate = {2023-04-17},
  date    = {2019-10-23},
  url     = {http://wiki.ros.org/roslaunch}
}

@online{ROScontributions,
  author  = {Open Robotics},
  title   = {ROS contributions},
  urldate = {2023-04-20},
  date    = {2021-05-15},
  url     = {http://wiki.ros.org/Distributions}
}








@online{ros-Packages,
  author  = {Open Robotics},
  title   = {Packages},
  urldate = {2023-05-21},
  date    = {2019-04-14},
  url     = {http://wiki.ros.org/Packages}
}

@online{rqt-graph,
  author  = {Open Robotics},
  title   = {rqt\_graph},
  urldate = {2023-05-21},
  date    = {2018-09-17},
  url     = {http://wiki.ros.org/rqt_graph}
}

@online{ros-bags,
  author  = {Open Robotics},
  title   = {Bags},
  urldate = {2023-05-21},
  date    = {2022-02-15},
  url     = {http://wiki.ros.org/Bags}
}

@online{parameter-server,
  author  = {Open Robotics},
  title   = {Parameter Server},
  urldate = {2023-05-21},
  date    = {2018-11-08},
  url     = {http://wiki.ros.org/Parameter%20Server}
}

@online{ros-master,
  author  = {Open Robotics},
  title   = {Master},
  urldate = {2023-05-21},
  date    = {2018-01-15},
  url     = {http://wiki.ros.org/Master}
}

@online{roslaunch,
  author  = {Open Robotics},
  title   = {roslaunch},
  urldate = {2023-05-21},
  url     = {http://wiki.ros.org/roslaunch}
}

@online{ros-tools,
  author  = {Open Robotics},
  title   = {Tools},
  urldate = {2023-05-21},
  date    = {2021-10-19},
  url     = {http://wiki.ros.org/Tools}
}

@online{client-libraries,
  author  = {Open Robotics},
  title   = {Client Libraries},
  urldate = {2023-05-21},
  date    = {2020-12-17},
  url     = {http://wiki.ros.org/Client%20Libraries}
}

@online{roscpp,
  author  = {Open Robotics},
  title   = {roscpp},
  urldate = {2023-05-21},
  date    = {2015-11-02},
  url     = {http://wiki.ros.org/roscpp}
}

@online{rospy,
  author  = {Open Robotics},
  title   = {rospy},
  urldate = {2023-05-21},
  date    = {2017-11-08},
  url     = {http://wiki.ros.org/rospy}
}

@online{SIL,
  title   = {What is software-in-the-loop testing?},
  urldate = {2023-05-21},
  date    = {2022-03-17},
  url     = {https://www.aptiv.com/en/insights/article/what-is-software-in-the-loop-testing}
}

@online{qGroundControl,
  author  = {QGroundControl},
  title   = {QGroundControl User Guide},
  urldate = {2023-04-20},
  url     = {https://docs.qgroundcontrol.com/master/en/index.html}
}

@online{Simulation,
  author  = {Gabler},
  title   = {Simulation},
  urldate = {2023-05-21},
  url     = {https://wirtschaftslexikon.gabler.de/definition/simulation-43833}
}

@online{gazebosim,
  author  = {Open Source Robotics Foundatio},
  title   = {Beginner: Overview},
  urldate = {2023-04-30},
  url     = {https://classic.gazebosim.org/tutorials?cat=guided_b&tut=guided_b1}
}

@online{mavlink,
  author  = {ArduPilot Dev Team},
  title   = {MAVLink Basics},
  urldate = {2023-05-06},
  date    = {2023},
  url     = {https://ardupilot.org/dev/docs/mavlink-basics.html}
}

@online{statischeSim,
  author  = {cae-wiki},
  title   = {Statische Simulation},
  urldate = {2023-05-16},
  date    = {2015-12-17},
  url     = {http://www.cae-wiki.info/wikiplus/index.php/Statische_Simulation}
}

@online{dynamischeSim,
  author  = {cae-wiki},
  title   = {Dynamische Simulation},
  urldate = {2023-05-16},
  date    = {2015-12-17},
  url     = {http://www.cae-wiki.info/wikiplus/index.php/Dynamische_Simulation}
}

@online{monte-carlo-sim,
  author = {IBM},
  title = {Was ist die Monte-Carlo-Simulation?},
  date = {2023-05-16},
  url = {https://www.ibm.com/de-de/topics/monte-carlo-simulation}
}

@online{system-dynamics,
  author = {wirtschaftslexikon24},
  title = {system dynamics},
  date = {2023-05-16},
  url = {https://www.wirtschaftslexikon24.com/d/system-dynamics/system-dynamics.htm}
}


@online{system-dynamics,
  author = {wirtschaftslexikon24},
  title = {system dynamics},
  date = {2023-05-16},
  url = {https://www.wirtschaftslexikon24.com/d/system-dynamics/system-dynamics.htm}
}


@online{flight-modes,
  author = {PX4 Autopilot},
  title = {Flight Modes},
  date = {2023-05-18},
  urldate = {2022-10-27},
  url = {https://docs.px4.io/main/en/concept/flight_modes.html}
}

@online{mavros,
  author = {COEX},
  title = {MAVROS},
  date = {2023-05-19},
  url = {https://clover.coex.tech/en/mavros.html}
}

@online{simple_offboard,
  author = {COEX},
  title = {Autonomous flight},
  date = {2023-05-19},
  url = {https://clover.coex.tech/en/simple_offboard.html}
}

@online{kinect_ros_driver,
  author = {christian-rauch,amelhassan,helenol},
  title = {Azure Kinect ROS Driver},
  date = {2023-05-19},
  url = {https://github.com/microsoft/Azure_Kinect_ROS_Driver}
}

@inproceedings{elasticfusion,
   abstract = {We present a novel approach to real-time dense visual SLAM. Our system is capable of capturing comprehensive dense globally consistent surfel-based maps of room scale environments explored using an RGB-D camera in an incremental online fashion, without pose graph optimisation or any post-processing steps. This is accomplished by using dense frame-to-model camera tracking and windowed surfel-based fusion coupled with frequent model refinement through non-rigid surface deformations. Our approach applies local model-to-model surface loop closure optimisations as often as possible to stay close to the mode of the map distribution, while utilising global loop closure to recover from arbitrary drift and maintain global consistency.},
   author = {Thomas Whelan and Stefan Leutenegger and Renato F. Salas-Moreno and Ben Glocker and Andrew J. Davison},
   doi = {10.15607/RSS.2015.XI.001},
   issn = {2330765X},
   journal = {Robotics: Science and Systems},
   title = {ElasticFusion: Dense SLAM without a pose graph},
   volume = {11},
   year = {2015},
}

@article{bescos2018dynaslam,
  title={{DynaSLAM}: Tracking, Mapping and Inpainting in Dynamic Environments},
  author={Bescos, Berta, F\'acil},
  journal={IEEE RA-L},
  year={2018}
 }
@article{ORBSLAM3TRO,
  title={{ORB-SLAM3}: An Accurate Open-Source Library for Visual, Visual-Inertial 
           and Multi-Map {SLAM}},
  author={Campos, Carlos AND Elvira, Richard AND Gomez, Juan J. AND Montiel, 
          Jos\'e M. M. AND Tard\'os, Juan D.},
  journal={IEEE Transactions on Robotics}, 
  volume={37},
  number={6},
  pages={1874-1890},
  year={2021}
 }


@online{coex_pix_fcu,
  author = {COEX},
  title = {COEX Pix},
  date = {2023-05-20},
  url = {https://clover.coex.tech/en/coex_pix.html}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%images
@online{compassCal,
  author  = {COEX},
  title   = {Sensor calibration},
  urldate = {2023-05-21},
  url     = {https://clover.coex.tech/assets/qgc-cal-compass.png}
}

@online{accCal,
  author  = {COEX},
  title   = {Sensor calibration},
  urldate = {2023-05-21},
  url     = {https://clover.coex.tech/assets/qgc-cal-acc.png}
}

@online{imgPidCal,
  author  = {PX4 Outopilot},
  title   = {Multicopter PID Tuning Guide},
  urldate = {2023-05-21},
  url     = {https://docs.px4.io/v1.12/assets/img/qgc_mc_pid_tuning_rate_controller.950965c8.png}
}

@online{imgcoexclover,
  author  = {ROS Robots},
  title   = {COEX Clover},
  urldate = {2023-02-06},
  url     = {https://robots.ros.org/clover/}
}

@online{imgcoexcloverkit,
  author  = {COEX},
  title   = {A comprehensive aerial robotics solution},
  urldate = {2023-02-08},
  url     = {https://coex.tech/clover}
}

@online{imgcoexpix,
  author  = {COEX},
  title   = {COEX Pix},
  urldate = {2023-05-20},
  url     = {https://clover.coex.tech/assets/coex_pix/coexpix-top.jpg}
}


@online{imgraspberrypi,
  author  = {Raspberry Pi},
  title   = {Raspberry Pi 4},
  urldate = {2023-03-17},
  url     = {https://www.raspberrypi.com/products/raspberry-pi-4-model-b/}
}

@online{imgfreiheitsgrade,
author = {GregorDS},
title = {Freiheitsgrade},
urldate = {2023-04-30},
url = {https://commons.wikimedia.org/wiki/File:6DOF.svg},
howpublished = {\url{https://creativecommons.org/licenses/by-sa/4.0/legalcode}}
}

@online{imgmavlink,
author = {ArduPilot Dev Team},
title = {MAVLink Basics},
urldate = {2023-05-06},
url = {https://ardupilot.org/dev/_images/mavlink-message-flow.png},
}

@online{CJMCU-531,
title = {A new generation, long distance ranging Time-of-Flight sensor based on ST’s FlightSense™ technology},
urldate = {2023-05-20},
date = {2018-11-xx},
url = {https://www.pololu.com/file/0J1506/VL53L1X.pdf},
}

@online{OV5647,
author = {Omni Vision},
title = {Datasheet OV5647},
urldate = {2023-05-20},
date = {2009-03-11},
url = {https://cdn.sparkfun.com/datasheets/Dev/RaspberryPi/ov5647_full.pdf},
}

@book{SWB-470983582,
title = {Machine vision : theory, algorithms, practicalities},
author = {Davies, E. R.},
address = {Amsterdam [u.a.]},
publisher = {Morgan Kaufmann},
year = {2005},
edition = {3. ed.},
}
@article{Yousif2015,
   abstract = {This paper is intended to pave the way for new researchers in the field of robotics and autonomous systems, particularly thosewho are interested in robot localization and mapping. We discuss the fundamentals of robot navigation requirements and provide a reviewof the state of the art tech- niques that form the bases of established solutions formobile robots localization and mapping. The topicswediscuss range from basic localization techniques such as wheel odometry and dead reckoning, to the more advance Visual Odometry (VO) and Simultaneous Localization and Mapping (SLAM) techniques. We discuss VO in both monocular and stereo vision systems using feature matching/tracking and optical flow techniques.We discuss and compare the basics of most common SLAM methods such as the Extended Kalman Fil- ter SLAM (EKF-SLAM), Particle Filter and the most recent RGB-D SLAM. We also provide techniques that form the building blocks to those methods such as feature extraction (i.e. SIFT, SURF, FAST), feature matching, outlier removal and data association techniques.},
   author = {Khalid Yousif and Alireza Bab-Hadiashar and Reza Hoseinnezhad},
   doi = {10.1007/s40903-015-0032-7},
   issn = {2363-6912},
   issue = {4},
   journal = {Intelligent Industrial Systems},
   title = {An Overview to Visual Odometry and Visual SLAM: Applications to Mobile Robotics},
   volume = {1},
   year = {2015},
}


@article{mattern-diskrete-simulation,
    author = "Friedemann Mattern and Horst Mehl",
    title = "Diskrete Simulation -- Prinzipien und Probleme der Effizienzsteigerung durch Parallelisierung",
    journal = "Informatik-Spektrum",
    year = "1989",
    pages = "198--210",
    volume = "12",
    number = "4"
}

@misc{SWB-1841134112,
title = {Multisensorielle Navigation und Kartierung in schwierigen Bedingungen},
author = {Christopher Doer},
address = {[Erscheinungsort nicht ermittelbar]},
publisher = {KIT Scientific Publishing},
year = {2023},
size = {1 Online-Ressource (240 p.)},
note = {German; Robust navigation as well as mapping of the environment are critical for autonomous aerial vehicles. This is especially challenging in poor visual conditions or degraded GNSS. Therefore, this thesis proposes multi-sensor fusion approaches based on FMCW radar, cameras and inertial sensors. These approaches can cope with such challenging conditions as demonstrated in different scenarios},
bestand = {Universitätsbibliothek Heidelberg <16>;
Karlsruher Institut für Technologie (KIT) - KIT-Bibliothek <90>;
Badische Landesbibliothek <31>;
Hochschulbibliothek Reutlingen <Rt 2> [Signatur: eBook];
Hochschule Zittau/Görlitz - Hochschulbibliothek <Zi 4>;
Hochschule Albstadt-Sigmaringen, Bibliothek Sigmaringen <991> [Signatur: eBook DOAB];
kostenfrei <LFER>;
Zentrum für Wissensmanagement, Bibliothek Hamm <1871> [Signatur: Open Access];
},
URL = {https://directory.doabooks.org/handle/20.500.12854/98565},
}
	
@article{luukkonen2011modelling,
  title={Modelling and control of quadcopter},
  author={Luukkonen, Teppo},
  journal={Independent research project in applied mathematics, Espoo},
  volume={22},
  number={22},
  year={2011}
}
@inproceedings{ragot:hal-04071273,
  TITLE = {{Benchmark of Visual SLAM Algorithms: ORB-SLAM2 vs RTAB-Map}},
  AUTHOR = {Ragot, Nicolas and Khemmar, Redouane and Pokala, Adithya and Rossi, Romain and Ertaud, Jean-Yves},
  URL = {https://hal-normandie-univ.archives-ouvertes.fr/hal-04071273},
  BOOKTITLE = {{2019 Eighth International Conference on Emerging Security Technologies (EST)}},
  ADDRESS = {Colchester, United Kingdom},
  PUBLISHER = {{IEEE}},
  PAGES = {1-6},
  YEAR = {2019},
  MONTH = Jul,
  DOI = {10.1109/EST.2019.8806213},
  KEYWORDS = {Visual SLAM ; ORB SLAM ; RTAB SLAM ; Localization ; Mapping ; Visual Odometry},
  PDF = {https://hal-normandie-univ.archives-ouvertes.fr/hal-04071273/file/EST_2019_KHEMMAR_RAGOT_ID_04.pdf},
  HAL_ID = {hal-04071273},
  HAL_VERSION = {v1},
}

@ARTICLE{9440682,
  author={Campos, Carlos and Elvira, Richard and Rodríguez, Juan J. Gómez and M. Montiel, José M. and D. Tardós, Juan},
  journal={IEEE Transactions on Robotics}, 
  title={ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual–Inertial, and Multimap SLAM}, 
  year={2021},
  volume={37},
  number={6},
  pages={1874-1890},
  doi={10.1109/TRO.2021.3075644}}


@Article{digital2030022,
AUTHOR = {Tychola, Kyriaki A. and Tsimperidis, Ioannis and Papakostas, George A.},
TITLE = {On 3D Reconstruction Using RGB-D Cameras},
JOURNAL = {Digital},
VOLUME = {2},
YEAR = {2022},
NUMBER = {3},
PAGES = {401--421},
URL = {https://www.mdpi.com/2673-6470/2/3/22},
ISSN = {2673-6470},
ABSTRACT = {The representation of the physical world is an issue that concerns the scientific community studying computer vision, more and more. Recently, research has focused on modern techniques and methods of photogrammetry and stereoscopy with the aim of reconstructing three-dimensional realistic models with high accuracy and metric information in a short time. In order to obtain data at a relatively low cost, various tools have been developed, such as depth cameras. RGB-D cameras are novel sensing systems that capture RGB images along with per-pixel depth information. This survey aims to describe RGB-D camera technology. We discuss the hardware and data acquisition process, in both static and dynamic environments. Depth map sensing techniques are described, focusing on their features, pros, cons, and limitations; emerging challenges and open issues to investigate are analyzed; and some countermeasures are described. In addition, the advantages, disadvantages, and limitations of RGB-D cameras in all aspects are also described critically. This survey will be useful for researchers who want to acquire, process, and analyze the data collected.},
DOI = {10.3390/digital2030022}
}

@INPROCEEDINGS{8310200,
  author={Bamji, Cyrus S. and Mehta, Swati and Thompson, Barry and Elkhatib, Tamer and Wurster, Stefan and Akkaya, Onur and Payne, Andrew and Godbaz, John and Fenton, Mike and Rajasekaran, Vijay and Prather, Larry and Nagaraja, Satya and Mogallapu, Vishali and Snow, Dane and McCauley, Rich and Mukadam, Mustansir and Agi, Iskender and McCarthy, Shaun and Xu, Zhanping and Perry, Travis and Qian, William and Chan, Vei-Han and Adepu, Prabhu and Ali, Gazi and Ahmed, Muneeb and Mukherjee, Aditya and Nayak, Sheethal and Gampell, Dave and Acharya, Sunil and Kordus, Lou and O'Connor, Pat},
  booktitle={2018 IEEE International Solid - State Circuits Conference - (ISSCC)}, 
  title={IMpixel 65nm BSI 320MHz demodulated TOF Image sensor with 3$\mu$m global shutter pixels and analog binning}, 
  year={2018},
  volume={},
  number={},
  pages={94-96},
  doi={10.1109/ISSCC.2018.8310200}}

 @Inbook{Valavanis2015,
author="Valavanis, Kimon P.
and Vachtsevanos, George J.",
editor="Valavanis, Kimon P.
and Vachtsevanos, George J.",
title="Sensors and Sensing Strategies: Introduction",
bookTitle="Handbook of Unmanned Aerial Vehicles",
year="2015",
publisher="Springer Netherlands",
address="Dordrecht",
pages="383--384",
abstract="Sensors and Sensing Strategies enable an unmanned aircraft to ``sense,'' ``see,'' ``hear,'' and ``understand'' the world around it so that it may function intelligently in an unknown and cluttered environment and in the absence of an onboard pilot. In essence, sensors and sensing strategies are crucial since they provide the technologies that will result in ``unmanned aircraft operating as if there were a human pilot onboard.''",
isbn="978-90-481-9707-1",
doi="10.1007/978-90-481-9707-1_135",
url="https://doi.org/10.1007/978-90-481-9707-1_135"
}

@misc{SWB-165930377X,
title = {Introduction to Multicopter Design and Control},
series = {SpringerLinkSpringer eBook Collection},
author = {Quan Quan},
address = {Singapore},
publisher = {Springer},
year = {2017},
isbn = {9789811033827},
size = {Online-Ressource (XXVI, 384 p. 205 illus., 128 illus. in color, online resource)},
keywords = {Quadrocopter / Flugverhalten / Flugregelung ; Aerospace engineering / Astronautics / Control engineering / Robotics / Automation / Engineering},
note = {; This book is the first textbook specially on multicopter systems in the world. It provides a comprehensive overview of multicopter systems, rather than focusing on a single method or technique. The fifteen chapters are divided into five parts, covering the topics of multicopter design, modeling, state estimation, control, and decision-making. It differs from other books in the field in three major respects: it is basic and practical, offering self-contained content and presenting hands-on methods; it is comprehensive and systematic; and it is timely. It is also closely related to the autopilot that users often employ today and provides insights into the code employed. As such, it offers a valuable resource for anyone interested in multicopters, including students, teachers, researchers, and engineers. Introduction -- PART I DESIGN -- Basic Composition -- Configuration and Structural Design -- Modeling and evaluation of propulsion system -- PART II MODELING -- Coordinate system and attitude representation -- Dynamic model and parameter identification -- PART III ESTIMATION -- Sensor model and calibration -- Observability and Kalman filter -- State estimation -- PART IV CONTROL -- Stability and Controllability -- Low-Level Flight Control -- Position Control based on SemiAutonomous Autopilot -- PART V DECISION -- Mission Decision-Making -- Health Evaluation and Failsafe -- Outlook},
bestand = {Bibliotheksservice-Zentrum <576>;
Sächsische Landesbibliothek - Staats- und Universitätsbibliothek <14>;
Karlsruher Institut für Technologie (KIT) - KIT-Bibliothek <90>;
Universitätsbibliothek Stuttgart <93>;
Technische Universität Chemnitz <Ch 1>;
Hochschulbibliothek Reutlingen <Rt 2> [Signatur: eBook];
Westsächsische Hochschule Zwickau <Zwi 2> [Signatur: Springer E-Book];
Bibliothek LIV Heilbronn Sontheim <840> [Signatur: Springer E-Book];
<Kon 4> Fachhochschule Konstanz <Kon 4> [Signatur: eBook Springer];
Hochschule Mannheim, Hochschulbibliothek <953> [Signatur: eBook Springer];
Fachhochschule Furtwangen <Fn 1> [Signatur: eBook Springer];
Hochschule Aalen <944> [Signatur: e-Book Springer];
Hochschule Ulm, Hochschule für Technik <943> [Signatur: eBook Springer];
FH Offenburg <Ofb 1> [Signatur: E-Book Springer];
Zentrum für Wissensmanagement, Bibliothek Hamm <1871> [Signatur: eBook Springer];
},
URL = {https://doi.org/10.1007/978-981-10-3382-7},
}



@misc{SWB-1681722674,
title = {RGB-D Image Analysis and Processing},
series = {Advances in Computer Vision and Pattern RecognitionSpringer eBooks},
editor = {Paul L. Rosin and Yu-Kun Lai and Ling Shao and Yonghuai Liu},
address = {Cham},
publisher = {Springer},
year = {2019},
edition = {1st ed. 2019},
isbn = {9783030286033},
size = {1 Online-Ressource (XI, 524 p. 178 illus., 152 illus. in color)},
keywords = { ; Image Processing and Computer Vision / Optical data processing / User interfaces (Computer systems) / Artificial intelligence},
note = {; Part I RGB-D Data Acquisition and Processing -- RGB-D Sensors: Data Acquisition -- Dealing with Missing Depth: Recent Advances in Depth Image Completion and Estimation -- Depth Super-resolution With Color Guidance: A Review -- RGB-D Sensors Data Quality Assessment and Improvement for Advanced Applications -- 3D Reconstruction from RGB-D Data -- RGB-D Odometry and SLAM -- Enhancing 3D Capture with Multiple Depth Camera Systems: A State-of-the-Art Report -- Part II RGB-D Data Analysis -- RGB-D Image-based Object Detection: from Traditional Methods to Deep Learning Techniques -- RGB-S Salient Object Detection: A Review -- Foreground Detection and segmentation in RGB-D Images -- Instance- and Category-level 6d Object Pose Estimation -- Part III RGB-D Applications -- Semantic RGB-D Perception for Cognitive Service Robots -- RGB-D Sensors and Signal Processing for Fall Detection -- RGB-D Interactive Systems on Serious Games for Motor Rehabilitation Therapy and Therapeutic Measurements -- Real-Time Hand Pose Estimation using Depth Camera.-RGB-D Object Classification for Autonomous Driving Perception -- People Counting in Crowded Environment and Re-identification -- References -- Index. This book focuses on the fundamentals and recent advances in RGB-D imaging as well as covering a range of RGB-D applications. The topics covered include: data acquisition, data quality assessment, filling holes, 3D reconstruction, SLAM, multiple depth camera systems, segmentation, object detection, salience detection, pose estimation, geometric modelling, fall detection, autonomous driving, motor rehabilitation therapy, people counting and cognitive service robots. The availability of cheap RGB-D sensors has led to an explosion over the last five years in the capture and application of colour plus depth data. The addition of depth data to regular RGB images vastly increases the range of applications, and has resulted in a demand for robust and real-time processing of RGB-D data. There remain many technical challenges, and RGB-D image processing is an ongoing research area. This book covers the full state of the art, and consists of a series of chapters by internationally renowned experts in the field. Each chapter is written so as to provide a detailed overview of that topic. RGB-D Image Analysis and Processing will enable both students and professional developers alike to quickly get up to speed with contemporary techniques, and apply RGB-D imaging in their own projects},
bestand = {Bibliotheksservice-Zentrum <576>;
Universitätsbibliothek Tübingen <21>;
Universität Konstanz, Kommunikations-, Informations-, Medienzentrum (KIM), <352E>;
Karlsruher Institut für Technologie (KIT) - KIT-Bibliothek <90>;
Universitätsbibliothek Stuttgart <93>;
Technische Universität Chemnitz <Ch 1>;
TU Bergakademie Freiberg, Universitätsbibliothek <105>;
Hochschulbibliothek Reutlingen <Rt 2> [Signatur: eBook];
Bibliothek LIV Heilbronn Sontheim <840> [Signatur: Springer E-Book];
<Kon 4> Fachhochschule Konstanz <Kon 4> [Signatur: eBook Springer];
Hochschule Mannheim, Hochschulbibliothek <953> [Signatur: eBook Springer];
Fachhochschule Furtwangen <Fn 1> [Signatur: eBook Springer];
Hochschule Aalen <944> [Signatur: E-Book Springer ];
Hochschule Ulm, Hochschule für Technik <943> [Signatur: eBook Springer];
Zentrum für Wissensmanagement, Bibliothek Hamm <1871> [Signatur: eBook Springer];
},
URL = {https://doi.org/10.1007/978-3-030-28603-3},
}
@article{Merzlyakov2021,
   abstract = {Advancing maturity in mobile and legged robotics technologies is changing the landscapes where robots are being deployed and found. This innovation calls for a transformation in simultaneous localization and mapping (SLAM) systems to support this new generation of service and consumer robots. No longer can traditionally robust 2D lidar systems dominate while robots are being deployed in multi-story indoor, outdoor unstructured, and urban domains with increasingly inexpensive stereo and RGB-D cameras. Visual SLAM (VSLAM) systems have been a topic of study for decades and a small number of openly available implementations have stood out: ORB-SLAM3, OpenVSLAM and RTABMap. This paper presents a comparison of these 3 modern, feature rich, and uniquely robust VSLAM techniques that have yet to be benchmarked against each other, using several different datasets spanning multiple domains negotiated by service robots. ORB-SLAM3 and OpenVSLAM each were not compared against at least one of these datasets previously in literature and we provide insight through this lens. This analysis is motivated to find general purpose, feature complete, and multi-domain VSLAM options to support a broad class of robot applications for integration into the new and improved ROS 2 Nav2 System as suitable alternatives to traditional 2D lidar solutions.},
   author = {Alexey Merzlyakov and Steve Macenski},
   month = {7},
   title = {A Comparison of Modern General-Purpose Visual SLAM Approaches},
   url = {http://arxiv.org/abs/2107.07589},
   year = {2021},
}	
@INPROCEEDINGS{6907054,
  author={Handa, Ankur and Whelan, Thomas and McDonald, John and Davison, Andrew J.},
  booktitle={2014 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={A benchmark for RGB-D visual odometry, 3D reconstruction and SLAM}, 
  year={2014},
  volume={},
  number={},
  pages={1524-1531},
  doi={10.1109/ICRA.2014.6907054}}
  
  @online{testdatatum,
  author={Technische Universität München},
  urldate=2023-05-20,
  url={https://cvg.cit.tum.de/data/datasets/rgbd-dataset/download},

  }