\chapter{Theoretische Grundlagen}

\section{Sensoren} \label{sensoren:section}

Sensoren sind entscheidend für die 3D Navigation anhand eines 3D Umweltmodells. Diese Sensoren werden verwendet, um präzise Informationen über die Umgebung eines Geräts zu erfassen, die dann zur Erstellung des Umweltmodells genutzt werden können. Zu den am häufigsten verwendeten Sensoren gehören Kameras, die RGB- und Tiefeninformationen erfassen, sowie LiDAR-Sensoren, die Entfernungsmessungen durchführen. Diese Sensoren werden in der Robotik, autonomen Fahrzeugen und anderen Anwendungen eingesetzt, um genaue Karten und Modelle der Umgebung zu erstellen, die für eine präzise Navigation erforderlich sind. Die Verwendung von Sensoren ermöglicht es Geräten, ihre Umgebung wahrzunehmen und sich darin zu orientieren, was für eine Vielzahl von Anwendungen von entscheidender Bedeutung ist.

    \subsection{Magnetometer} \label{magnetometer:subsection}

    Bei Magnetometern handelt es sich um Sensoren, die das Magnetfeld im Umfeld des Sensors messen können.
    Diese können sowohl in der Luft, als auch auf dem Boden eingesetzt werden.
    Die Sensoren sind in der Lage, das Magnetfeld in drei Dimensionen zu messen und darzustellen.
    Magnetometrie ist ein wichtiges Instrument in der Navigation, insbesondere in der inertialen Navigation und im autonomen Fahren, wo die genaue Bestimmung der Fahrzeugorientierung entscheidend ist. Es gibt verschiedene Arten von Magnetometern, wie Hall-Sensoren, Fluxgate-Sensoren und Magnetoresistive-Sensoren, die alle auf unterschiedlichen physikalischen Prinzipien basieren. Diese Sensoren können sowohl auf der Erdoberfläche als auch in der Luft eingesetzt werden, um das magnetische Feld zu messen.

    Die Messung des Magnetfeldes erfolgt dabei in der Regel über einen Halbleiter, der durch das Magnetfeld beeinflusst wird und einer Elektronik, die das Signal aufbereitet und ausgibt.
    Magnetometer können jedoch nur die Richtung des Magnetfeldes messen, jedoch nicht die Stärke des Magnetfeldes.
    Magnetometer werden verwendet, um die Ausrichtung von Geräten in bestimmten Koordinatensystemen zu bestimmen.
    Sie können für diese Ausrichtungsbestimmung auch in der Luft verwendet werden. 
 
    \subsection{Abstandssensoren} \label{abstandssensoren:subsection}

    Distanzsensoren sind Sensoren, die den Abstand oder die Nähe eines Objekts zu einem anderen Objekt oder einer Oberfläche messen. Es gibt verschiedene Arten von Abstandssensoren, einschließlich Ultraschallsensoren, Infrarotsensoren, Lasersensoren und Laufzeitsensoren. Diese Sensoren werden in verschiedenen Anwendungen eingesetzt, z. B. in der Robotik, in der Automobilindustrie, in der industriellen Automatisierung, in Überwachungssystemen und in Navigationssystemen. Abstandssensoren spielen auch eine wichtige Rolle in der Robotik und Autonomie, insbesondere bei der Umgebungswahrnehmung, Navigation und Hindernisvermeidung. In dieser Studienarbeit werden Tiefenbildkameras und Lasersensoren für die Abstandsbestimmung verwendet.


    \subsection{Lasersensor}
    \label{chp:lasersensor}

    Ein Laser-Entfernungsmesser ist ein optoelektronisches Messinstrument, das zur Bestimmung der Entfernung zwischen einem Sender und einem Zielobjekt eingesetzt wird. Das grundlegende Prinzip der Messung beruht auf der Messung der Laufzeit eines ausgesandten Laserpulses, der auf das Zielobjekt trifft und reflektiert wird. Die Laufzeit des Laserpulses wird dann in eine Entfernung umgerechnet.
    Die Messung erfolgt durch das Senden eines kurzen Laserimpulses auf das Zielobjekt, dessen Reflexion vom Empfänger des Entfernungsmessers aufgefangen wird. Die Zeit, die der Laserimpuls benötigt, um das Zielobjekt zu erreichen und zurückzukehren, wird gemessen und in eine Entfernung umgerechnet. Dabei wird die Laufzeit des Laserpulses mit der Lichtgeschwindigkeit multipliziert und durch zwei geteilt, um die Entfernung zum Zielobjekt zu bestimmen.
    \begin{center}
    \label{math:distance}
    $Entfernung = \frac{Laufzeit\ des\ Laserimpulses\ x\ Lichtgeschwindigkeit}{2}$
    \end{center}


    \subsection{RGBD Kameras}

    Infrarot-Tiefenbildkameras, die auch als RGB-D-Kameras (RGB-Color-Depth) bekannt sind, sind optoelektronische Geräte, die zur Erfassung von Bildern verwendet werden und zusätzlich Tiefeninformationen liefern. Diese Kameras arbeiten mit einem optischen System, das einen projizierten Infrarotstrahl auf das zu erfassende Objekt lenkt und mit einer speziellen Kamera dessen Reflexionen erfasst. Auf diese Weise wird eine räumliche Tiefeninformation erstellt, die als Tiefenkarte bezeichnet wird.

    Die Funktionsweise einer \ac{RGB-D} Kamera basiert auf dem Prinzip der Laufzeitmessung auch \ac{ToF} genannt.  Dabei wird ein Lichtstrahl von einem Infrarotsender auf ein Objekt projiziert. Das Licht wird von der Oberfläche des Objekts reflektiert und von einem Infrarot-Empfänger in der Kamera aufgefangen.
    
    Da die Geschwindigkeit des Lichts konstant ist, kann die Entfernung zwischen Kamera und Objekt durch Messung der Flugzeit des Lichtstrahls berechnet werden.
    Die Zeit, die das Licht vom Sender zum Empfänger benötigt, wird gemessen und als "Laufzeit" bezeichnet.
    Die Entfernung berechnet sich nach der gleichen Formel \ref{math:distance} wie auch beim Laser-Entfernungsmesser. 


    Bei \ac{RGB-D} Kameras handelt es sich um Kameras, die ein Strukturlichtverfahren verwenden.
    Bei einem Strukturlichtverfahren werden Lichtmuster auf das zu rekonstruierende Projekt projeziert um das zusätzlich aufgenommene RGB Bild mit Tiefeninformationen zu versehen. Das in Kapitel \ref{chp:lasersensor} verwendete Prinzip wird auf das gesamte Bild angewendet. 
    
    Mithilfe der so gewonnenen Entfernungsinformationen wird eine Tiefenkarte erstellt, die eine 3D Rekonstruktion der Umgebung ermöglicht.

    
    
   Infrarot-Tiefenkameras können auch Farbinformationen aufnehmen, indem sie eine RGB-Kamera in das System integrieren. Durch die Kombination der Tiefenkarte mit der Farbinformation entsteht ein RGB-D-Bild, das die Form und die Farbe des Objekts in Echtzeit darstellt.

    Um eine Infrarot-Tiefenkamera in ein System zur Positionsbestimmung oder Umgebungserfassung einzubinden, müssen sowohl die intrinsischen als auch die extrinsischen Parameter bekannt sein. Die intrinsischen Parameter beschreiben die Eigenschaften der Kamera selbst, wie zum Beispiel die Brennweite oder die Verzerrungen, die durch die Linse entstehen. Die extrinsischen Parameter beschreiben die Beziehung zwischen der Kamera und dem Objekt oder der Umgebung, in der sie sich befindet. Hierzu gehören der Abstand, die Ausrichtung und die Position der Kamera im Raum.

    Die Kenntnis beider Parameter ist unerlässlich, um die Tiefenkarte und das RGB-Bild präzise miteinander zu verknüpfen und eine genaue 3D-Rekonstruktion der Umgebung zu erstellen. Die extrinsischen Parameter können durch Kalibrierung der Kamera relativ einfach ermittelt werden, indem bekannte Referenzobjekte im Raum angepeilt werden und die Positionen in der Tiefenkarte und im RGB-Bild verglichen werden. Die intrinsischen Parameter hingegen müssen oft aufwändiger bestimmt werden, beispielsweise durch Analyse von Mustern auf einer Kalibrierungsplatte.
    
    Insgesamt sind Infrarot-Tiefenkameras aufgrund ihrer Fähigkeit zur Erstellung von 3D-Punktwolken und zur gleichzeitigen Erfassung von Farbinformationen eine wertvolle Komponente in verschiedenen Anwendungen wie Robotik, Augmented Reality oder autonomen Fahrzeugen. 

    \cite[vgl.][Kapitel 3]{Tychola2022}

\subsection{Vergleich}\label{chp:depth-sensor-compar}
    Infrarot-Tiefenkameras (RGB-D-Kameras) und Laser-Entfernungsmesser finden auch Anwendung im Bereich der Drohnentechnologie.

Infrarot-Tiefenkameras können beispielsweise in Drohnen eingesetzt werden, um eine präzise Hinderniserkennung und -vermeidung zu ermöglichen. Die Kameras können die räumliche Tiefe von Objekten erfassen und somit eine zuverlässige Entfernungsmessung durchführen. Damit kann die Drohne selbstständig Hindernisse erkennen und ausweichen, was insbesondere in unübersichtlichem Gelände oder in der Indoor-Navigation von Vorteil ist.

ToF-Messungen haben in der Regel eine größere Blickfeldbreite als Laser Entfernungsmessungen, was bedeutet, dass sie ein größeres Sichtfeld abdecken können. Dies liegt daran, dass ToF-Messungen Infrarotlicht verwenden, das breiter streut als Laserlicht. Dadurch können ToF-Sensoren größere Bereiche in einem einzigen Messvorgang abdecken und somit mehr Inhalte aufnehmen.

Darüber hinaus haben ToF-Sensoren in der Regel auch eine höhere Bildrate als Laser-Entfernungsmesser. Dies bedeutet, dass sie schneller und kontinuierlicher messen können, was für Anwendungen wie Bewegungserkennung und -verfolgung nützlich sein kann.

Laser-Entfernungsmesser werden häufig eingesetzt, um präzise Entfernungen zu messen, wie beispielsweise bei der Vermessung von Land oder Gebäuden. In der Drohnentechnologie können Laser-Entfernungsmesser eingesetzt werden, um präzise Landungen und Abflüge durchzuführen oder um die exakte Position der Drohne zu bestimmen.




\section{ROS - Robot Operating System} \label{ros:section}
In den vergangenen Jahren hat die Wissenschaft im Bereich der Robotik enorme Fortschritte gemacht. Die Verfügbarkeit zuverlässiger und kostengünstiger Roboterhardware, angefangen bei mobilen Bodenrobotern über Quadrotor-Hubschrauber bis hin zu humanoiden Robotern, ist heute größer als jemals zuvor. Noch beeindruckender ist jedoch die Tatsache, dass Algorithmen entwickelt wurden, die diesen Robotern einen immer höheren Grad an Autonomie verleihen.

Trotz dieser rasanten Fortschritte stehen Softwareentwickler jedoch noch immer vor großen Herausforderungen bei der Programmierung von Robotern. Die Komplexität der Aufgaben, die von Robotern ausgeführt werden können, erfordert eine Menge an spezialisiertem Wissen und Erfahrung in verschiedenen Bereichen wie Sensorik, Bildverarbeitung, Künstlicher Intelligenz und Robotik.

Die Igeneure und Softwareentwickler haben jedoch die Herausforderungen erkannt und arbeitet intensiv daran, sie zu lösen. Neue Werkzeuge und Frameworks werden entwickelt, um Entwicklern zu helfen, Roboter schneller und effektiver zu programmieren. Eines der bekanntensten Tools in diesem Bereich ist das Robot Operating System \ac{ROS}

\ac{ROS} ist eine Open-Source-Plattform, die speziell für die Entwicklung von Robotersoftware entwickelt wurde. Es bietet eine Reihe von Bibliotheken, Tools und Frameworks, die es Entwicklern ermöglichen, komplexe Robotikanwendungen zu erstellen und zu betreiben. \ac{ROS} wurde von Willow Garage entwickelt und ist heute ein weit verbreitetes Framework in der Robotik-Community.

\ac{ROS} besteht aus verschiedenen Modulen, die es ermöglichen, Roboterhard-  und software zu abstrahieren und zu standardisieren. Die Plattform bietet eine Vielzahl von Werkzeugen für die Entwicklung von Robotik-Software, einschließlich Visualisierungstools, Datenverarbeitungs- und Analysetools sowie eine umfassende Dokumentation.

\ac{ROS} ist so konzipiert, dass es auf einer Vielzahl von Betriebssystemen und Hardwarearchitekturen laufen kann und bietet Unterstützung für eine breite Palette von Robotern und Sensoren. Es ist auch bekannt für seine Fähigkeit zur Zusammenarbeit zwischen verschiedenen Robotern, die miteinander kommunizieren und Aufgaben gemeinsam erledigen können.

Dank seiner leistungsstarken Funktionen und Flexibilität ist \ac{ROS} zu einem der wichtigsten Frameworks für die Robotik-Entwicklung geworden und wird in vielen Anwendungen eingesetzt, von industriellen Robotern bis hin zu autonomen Fahrzeugen.

    \subsection{Vorraussetzungen zur Verwendung von ROS} \label{Vorraussetzungen zur Verwendung von ROS:subsection}
    \ac{ROS} kann man nicht auf jedem Betriebssystem verwenden. Lauffähig ist es nur auf Unix basierenden Systemen. Am besten funktioniert hierbei Linux und hierbei Ubuntu. Aber auch auf Mac OS kann man ROS verwenden. Allerdings ist die Version für Mac OS bislange nur experimentell. Unter Microsoft Windows direkt kann \ac{ROS} nicht verwenden. Allerdings kann man durch die Verwendung von Docker oder einer virtuellen Maschiene mit z.B. Linux als Gastsystem auch von Windows Computern \ac{ROS} verwenden \cite[vgl.][]{ROSIntroduction}.

    Die \ac{ROS} Version, welche für diese Arbeit verwendet wurde, ist die Version "ROS Noetic Ninjemys". Diese ist die neuste ROS 1 Version mit long term support. Das Betriebssystem, auf welches die Version abzielt, ist Ubuntu 20.04 (Focal). Dementsprechend wurde auch Ubuntu 20.04 für diese Arbeit verwendet.

    \subsection{Ebenen von ROS} \label{Ebenen von ROS:subsection}
    Wie in \cite{ROSconcepts}
    \todo{schreiben?}

    \subsection{ROS-Distributionen} \label{ROS-Distributionen:subsection}
    Eine ROS-Distribution besteht aus einem festgelegten und visionierten Set von ROS-Paketen. Vergelichbar ist dieses System mit den Linux-Distributionen wie z.B. Ubuntu. Der Hauptzweck von ROS-Distributionen besteht darin, Entwicklern eine relativ stabile Codebasis zu bieten, damit sie daran weiterarbeiten können. Sobald eine Distribution veröffentlicht wurde, werden Änderungen hauptsächlich auf Fehlerbehebungen und nicht-zerstörerische Verbesserungen für die Kernpakete (alles unter ros-desktop-full) beschränkt. Somit ist das "Umziehen" auf eine neue Distribution weniger Fehleranfällig und einfacher. Dies gilt im Allgemeinen für die gesamte ROS-Community, jedoch sind die Regeln für "höhere" Pakete weniger streng und somit liegt es an den Verantwortlichen eines spezifischen Pakets, zerstörende Änderungen zu vermeiden \cite[vgl.][]{ROScontributions}.

    \subsection{Nodes} \label{nodes:subsection}
    In \ac{ROS} werden Funktionen und Prozesse durch sogenannte Nodes realisiert. Eine Node ist eine ausführbare Einheit, die in einem \ac{ROS}-System arbeitet und über eine eindeutige Identifikation verfügt. Jede Node hat eine spezifische Aufgabe, wie beispielsweise das Sammeln von Sensordaten, die Ausführung einer spezifischen Berechnung oder das Steuern eines Aktors.

    Nodes können miteinander kommunizieren, indem sie Nachrichten senden und empfangen. Nachrichten sind definierte Datenstrukturen, die Informationen zwischen Nodes transportieren. Nodes können auch Services anbieten oder anfordern, um eine bestimmte Aktion auszuführen.

    Eine wichtige Funktion von Nodes ist ihre Fähigkeit zur Verteilung. In \ac{ROS} können Nodes auf verschiedenen Hosts oder in verschiedenen Prozessen ausgeführt werden. Dadurch können komplexe \ac{ROS}-Systeme erstellt werden, die aus vielen miteinander verbundenen Nodes bestehen.

    Nodes können auch in einer \ac{ROS}-Graphenstruktur organisiert werden. Diese Struktur zeigt die Abhängigkeiten zwischen Nodes und die Art der Kommunikation zwischen ihnen an. Die \ac{ROS}-Graphenstruktur kann mit Werkzeugen wie "rqt\_graph" visualisiert werden, um eine bessere Übersicht über das System zu erhalten.

    Die Verwendung von Nodes in \ac{ROS} ermöglicht eine hohe Flexibilität und Modularität bei der Entwicklung von Robotik-Anwendungen. Entwickler können einzelne Nodes erstellen, testen und optimieren, bevor sie sie in einem größeren System einsetzen. Darüber hinaus können Nodes wiederverwendet werden, um ähnliche Funktionen in verschiedenen Anwendungen auszuführen.

    Insgesamt sind Nodes eine zentrale Komponente von \ac{ROS} und ermöglichen es Entwicklern, komplexe Roboteranwendungen mit einer hohen Flexibilität und Modularität zu erstellen.

    \subsection{Topics} \label{topics:subsection}
    In \ac{ROS} werden Daten zwischen Nodes durch sogenannte Topics ausgetauscht. Ein Topic ist eine benannte Kommunikationsleitung, über die Nodes Nachrichten senden und empfangen können. Topics ermöglichen die einfache und flexible Kommunikation zwischen Nodes, ohne dass die Nodes über die genaue Identität des Empfängers Bescheid wissen müssen.

    Ein Topic hat einen bestimmten Datentyp, der definiert, welche Art von Daten zwischen Nodes ausgetauscht werden können. Es können beispielsweise Sensordaten wie Bilder oder Entfernungsmessungen, oder Steuerbefehle für Aktoren wie Motoren oder Greifer übertragen werden.

    Nodes können sich auf ein Topic abonnieren, um die Nachrichten, die auf diesem Topic veröffentlicht werden, zu empfangen. Jedes Mal, wenn eine Nachricht auf einem Topic veröffentlicht wird, wird sie an alle Nodes weitergeleitet, die auf dieses Topic abonniert sind.

    Topics können auch von Nodes veröffentlicht werden, um Nachrichten an andere Nodes zu senden. Eine Node, die ein Topic veröffentlicht, wird als Publisher bezeichnet. Der Publisher kann regelmäßig Nachrichten auf einem Topic veröffentlichen, um andere Nodes über Änderungen in der Umgebung oder im System zu informieren.

    Die Verwendung von Topics in \ac{ROS} ermöglicht eine einfache und flexible Kommunikation zwischen Nodes, was besonders in komplexen Systemen von Vorteil ist. Nodes können sich auf mehrere Topics abonnieren und Nachrichten an mehrere Topics veröffentlichen, was eine effektive und modulare Datenverarbeitung ermöglicht. Zudem können Topics auf mehreren Hosts oder in verschiedenen Prozessen ausgeführt werden, was eine Skalierung des \ac{ROS}-Systems ermöglicht.

    Insgesamt sind Topics eine wichtige Komponente von \ac{ROS} und ermöglichen es Entwicklern, eine einfache und effektive Kommunikation zwischen Nodes in Robotik-Anwendungen zu realisieren.

    \subsection{Messages} \label{messages:subsection}
    Die Messages sind Datenströme, die zwischen mindestens zwei Nodes ausgetauscht werden. Sie werden in sogenannten msg-Dateien definiert. Diese Nachrichten können sowohl aus primitiven Datentypen als auch aus Datenstrukturen bestehen. Der Datenfluss erfolgt immer nur in eine Richtung.

    \subsection{Publish and Subscripe Pattern} \label{publish_and_subscripe_pattern:subsection}
    Das Publish-Subscribe-Pattern ist ein grundlegendes Muster der \ac{ROS}-Kommunikation und ermöglicht eine effektive und modulare Datenverarbeitung in verteilten Systemen.

    Beim Publish-Subscribe-Pattern senden Nodes, die Informationen über eine bestimmte Ressource verarbeiten, die Informationen an ein Topic, das als Vermittler dient. Nodes, die an den Informationen interessiert sind, abonnieren das Topic und erhalten alle zukünftigen Nachrichten, die von Nodes veröffentlicht werden, die mit dem Topic verbunden sind.

    Dieses Muster hat mehrere Vorteile. Zum einen ermöglicht es eine flexible Architektur, in der Nodes unabhängig voneinander arbeiten und sich auf das Abonnieren und Veröffentlichen von Topics konzentrieren können, ohne die genaue Identität des Empfängers oder Senders zu kennen. Zum anderen ermöglicht es eine effektive Datenverarbeitung, da mehrere Nodes dieselben Informationen von einem Publisher erhalten können.

    Ein weiterer Vorteil des Publish-Subscribe-Patterns ist, dass es eine einfache Möglichkeit bietet, den Zustand von Ressourcen zu überwachen oder auf Änderungen in Echtzeit zu reagieren. So kann beispielsweise eine Node, die eine Kamera überwacht, die Bilder auf einem Topic veröffentlichen. Andere Nodes, die an der Verarbeitung dieser Bilder beteiligt sind, können sich auf das Topic abonnieren und die Informationen in Echtzeit verarbeiten.

    Das Publish-Subscribe-Pattern ist ein grundlegendes Konzept in \ac{ROS} und wird in der Regel für die Kommunikation zwischen Nodes verwendet. Es ermöglicht eine effektive und modulare Datenverarbeitung in verteilten Systemen und ist ein wesentlicher Bestandteil von \ac{ROS}, um komplexe Robotik-Anwendungen zu realisieren.

    \subsection{Objekterkennung} \label{objekterkennung:subsection}
    In \ac{ROS} gibt es verschiedene Methoden zur Objekterkennung, die in Robotik-Anwendungen eingesetzt werden können. Die Objekterkennung ist ein wichtiger Schritt in der automatisierten Wahrnehmung von Robotern, da sie es ihnen ermöglicht, ihre Umgebung zu verstehen und darauf zu reagieren.

    Eine häufig verwendete Methode zur Objekterkennung in \ac{ROS} ist die Verwendung von 3D-Sensoren wie Lidar oder Kinect. Diese Sensoren erfassen Daten über die Umgebung des Roboters und können dabei helfen, Objekte zu identifizieren und ihre Position und Orientierung im Raum zu bestimmen.
    \todo{Kinect kein Sensor System}
    Eine weitere Methode zur Objekterkennung in \ac{ROS} ist die Verwendung von Bildverarbeitungs-Algorithmen. Dabei können beispielsweise Farb- oder Formmerkmale verwendet werden, um Objekte in Bildern zu erkennen und ihre Position und Ausrichtung zu bestimmen.

    Eine weiterentwickelte Methode zur Objekterkennung in \ac{ROS} ist die Verwendung von Deep-Learning-Methoden wie Convolutional Neural Networks (CNNs). Dabei werden CNNs trainiert, um Objekte in Bildern oder Punktwolken zu erkennen und zu klassifizieren. Diese Methode erfordert jedoch ein umfangreiches Training und eine hohe Rechenleistung, um in Echtzeit ausgeführt zu werden.

    Die Objekterkennung ist ein wichtiger Schritt in der automatisierten Wahrnehmung von Robotern, da sie es ihnen ermöglicht, ihre Umgebung zu verstehen und darauf zu reagieren. \ac{ROS} bietet verschiedene Methoden zur Objekterkennung, die in Robotik-Anwendungen eingesetzt werden können. Die Wahl der richtigen Methode hängt von den Anforderungen der Anwendung ab und erfordert oft eine sorgfältige Abwägung zwischen Genauigkeit, Geschwindigkeit und Komplexität.

    \subsection{QR-Codes} \label{qr-codes:subsection}
    \ac{ROS} bietet verschiedene Möglichkeiten zur Erkennung von QR-Codes in Robotik-Anwendungen. QR-Codes sind zweidimensionale Barcodes, die Informationen wie URLs, Texte oder andere Daten enthalten können. Die Erkennung von QR-Codes kann in \ac{ROS}-basierten Anwendungen genutzt werden, um Informationen zu lesen, Roboter zu navigieren oder um eine Interaktion mit der Umgebung zu ermöglichen.

    Es gibt verschiedene \ac{ROS}-Pakete, die die QR-Code-Erkennung erleichtern. Ein Beispiel ist das "zbar\_ros" Paket, das ein Wrapper für die Open-Source-ZBar-Bibliothek ist, die QR-Codes und andere Barcodes erkennt. Das zbar\_ros-Paket ermöglicht es, den Inhalt von QR-Codes aus dem Kamerabild zu extrahieren und als \ac{ROS}-Topic zu veröffentlichen, der von anderen Nodes abonniert werden kann. Das Paket bietet auch Optionen zur Konfiguration der QR-Code-Erkennung, wie beispielsweise die Festlegung der Mindestgröße des Codes oder die Einstellung der Scan-Frequenz.

    Ein weiteres \ac{ROS}-Paket, das die QR-Code-Erkennung erleichtert, ist das "ros\_qr\_detector" Paket. Dieses Paket basiert auf der OpenCV-Bibliothek und bietet eine einfache Möglichkeit, QR-Codes in \ac{ROS}-basierten Anwendungen zu erkennen. Das Paket bietet auch die Möglichkeit, QR-Codes aus der Kamerabildanzeige auszuschneiden und als separate Bilder zu speichern.

    Die Erkennung von QR-Codes in \ac{ROS}-Anwendungen kann für verschiedene Anwendungsfälle nützlich sein. Beispielsweise können QR-Codes als Marker verwendet werden

    \subsection{Starten eines ROS Programms} \label{starten eines ROS Programms:subsection}
    Zum Starten verschiedener Nodes in \ac{ROS} wird das ROS-eigene Tool roslaunch verwendet. Roslaunch ist ein mächtiges Werkzeug für die Konfiguration, das Starten und die Überwachung von ROS-Paketen und -Nodes. Mit roslaunch können Entwickler schnell und einfach Nodes starten, indem sie eine XML-Datei erstellen, die alle notwendigen Konfigurationen enthält. Die XML-Datei wird als Launch-Datei bezeichnet und wird in der Regel von Entwicklern erstellt, um eine oder mehrere ROS-Knoten gleichzeitig zu starten und zu konfigurieren.

    Ein großer Vorteil von roslaunch ist seine Portabilität. Mit Launch-Dateien können Entwickler ROS-Pakete und -Nodes problemlos auf verschiedenen Systemen starten und konfigurieren, unabhängig von der Plattform oder Architektur. Dies erleichtert die Entwicklung von ROS-Software für verschiedene Roboter- und Hardware-Plattformen erheblich. \cite{roslaunch}

    Der Befehl, mit welchem man roslauch aufruft, sieht wie folgt aus:
    
    \textit{\$ roslaunch package\_name file.launch}

\section{Multicopter} \label{drohne:section}

Multicopter sind eine Form von Drohnen, auch bekannt als \ac{UAV}s. Sie sind eine aufstrebende Technologie, die in einer Vielzahl von Anwendungen eingesetzt werden kann. Eine der wichtigsten Anwendungen ist die Überwachung und Kartierung von schwer zugänglichen Gebieten. Drohnen können mit hochauflösenden Kameras und Sensoren ausgestattet werden, um genaue Bilder und Daten zu liefern, die für die Erstellung von Karten und für die Beobachtung von Flora und Fauna verwendet werden können.

Ein weiterer wichtiger Anwendungsbereich von Drohnen ist die Inspektion von Infrastrukturen wie Brücken, Windkraftanlagen und Pipelines. Drohnen können schnell und effektiv in schwer zugängliche Bereiche fliegen und visuelle Inspektionen durchführen, ohne dass ein Inspektor vor Ort sein muss. Dies kann die Inspektionszeit erheblich reduzieren und die Sicherheit für die Inspektoren erhöhen.

Darüber hinaus können Drohnen auch für die Lieferung von Waren eingesetzt werden. Dies kann eine schnelle und effiziente Möglichkeit sein, Pakete und andere Waren zu transportieren, insbesondere in Gebieten mit schlechter Infrastruktur. Einige Unternehmen haben bereits begonnen, Drohnen für die Lieferung von Waren zu testen und es wird erwartet, dass diese Anwendung in Zukunft weiter wachsen wird.

Insgesamt bieten Drohnen eine Vielzahl von Anwendungsmöglichkeiten und werden voraussichtlich in Zukunft eine noch wichtigere Rolle spielen. Es ist jedoch wichtig, dass angemessene Regulierungsrahmen und technologische Verbesserungen vorhanden sind, um sicherzustellen, dass Drohnen sicher und effektiv eingesetzt werden können.

Multicopter, auch bekannt als Quadrocopter, sind eine Art von Drohne, die durch vier Rotoren stabilisiert und gesteuert werden. Die Rotoren können unabhängig voneinander gesteuert werden, um Aufstieg, Abstieg, Drehung und Vorwärts- und Rückwärtsbewegungen zu ermöglichen. Die Regelung und Positionierung von Multicoptern ist eine komplexe Angelegenheit, die eine Kombination aus Sensoren, Steuerungsalgorithmen und Regelungssystemen erfordert. 
Der Flugcontroller ist das Herzstück des Regelungssystems und ist in der Lage, die Daten der Sensoren zu verarbeiten und die Steuerung des Multicopters anzupassen, um eine stabile Fluglage und eine genaue Positionierung zu gewährleisten. Die Steuerungsalgorithmen, die im Flugcontroller verwendet werden, können komplex sein und beinhalten oft eine Kombination aus PID-Reglern (Proportional-Integral-Derivative) und anderen Regelungstechniken.
Der Flugcontroller ist das Herzstück des Regelungssystems und ist in der Lage, die Daten der Sensoren zu verarbeiten und die Steuerung des Multicopters anzupassen, um eine stabile Fluglage und eine genaue Positionierung zu gewährleisten. Die Steuerungsalgorithmen, die im Flugcontroller verwendet werden, können komplex sein und beinhalten oft eine Kombination aus \ac{PID}-Reglern und anderen Regelungstechniken.

Der Flugcontroller ist das Herzstück des Regelungssystems und ist in der Lage, die Daten der Sensoren zu verarbeiten und die Steuerung des Multicopters anzupassen, um eine stabile Fluglage und eine genaue Positionierung zu gewährleisten. Die Steuerungsalgorithmen, die im Flugcontroller verwendet werden, können komplex sein und beinhalten oft eine Kombination aus PID-Reglern (Proportional-Integral-Derivative) und anderen Regelungstechniken.

\section{Regelsysteme} \label{regelsysteme:section}


    \subsection{P-Regler}

    
    Ein \ac{P-Regler} ist eine grundlegende Form der Regelungstechnik, die oft in industriellen Anwendungen verwendet wird. Der \ac{P-Regler} basiert auf dem Prinzip der proportionalen Rückkopplung. Er wirkt, indem der Regler die Differenz zwischen dem Ist und Sollwert signalisiert und dann eine proportionale Regelgröße zum Fehler signalisiert.
    Es gibt eine direkte proportionale Beziehung zwischen der Ausgangsleistung des Reglers und dem Fehler. Die mathematische Formel für den P-Regler ist: 

    \begin{figure}[H]
    \begin{equation*}
        u(t) = K_p e(t)
    \end{equation*}
    \eqlabel{eq:p-regler}{P-Regler}

\end{figure}

    wo $u(t)$ das Signal des Reglers, $K_p$ der Proportionalitätsfaktor und $e(t)$ der Fehler zwischen der Ist- und Sollwert der Regelgrößen ist.

    \subsection{I-Regler}

    Neben \ac{P-Regler}n gibt es \ac{I-Regler}.
    Aus dem Name \ac{I-Regler}, lässt sich das Wort \textit{Integral} ableiten. Ein \ac{I-Regler} ist eine Form der Regelungstechnik, die verwendet wird, um den Regelgrößenfehler zwischen Soll- und Istwert zu minimieren und das System auf den Sollwert zu bringen. 

    Der I-Regler basiert auf dem Prinzip der Integration des Regelgrößenfehlers über die Zeit. Die mathematische Gleichung für den I-Regler ist:

    \begin{figure}[H]
        \begin{equation*}
            u(t) = K_i \int_{0}^{t} e(\tau) d\tau
        \end{equation*}
    \end{figure}

    wo $u(t)$ das Signal des Reglers, $K_i$ der Integrationsfaktor und $e(t)$ der Regelgrößenfehler ist. Der Integrationsfaktor ist ein Maß für die Empfindlichkeit des Reglers auf den Regelgrößenfehler.

Durch einen höheren $K_i$ Faktor reagiert der Regler stärker, führt jedoch auch zu langsameren Reaktionszeiten des Reglers.

    Das \ac{I-Glied} summiert Regeldifferenzen auf, dies resultiert darin, dass die Regeldifferenz im geschlossenen Regelkreis gegen den Wert 0 läuft.



    \subsection{D-Regler}
    
    Ein weiterer wichtiger Regler ist der \ac{D-Regler}. Ein \ac{D-Regler} wird verwendet um den Regelgrößefehler zwischen Soll- und Istwert zu minimieren.

    Der \ac{D-Regler} wird verwendet, um das System zu stabilisieren und auf Störungen zu reagieren. Die mathematische Gleichung für den D-Regler lautet:

    \begin{figure}
        \begin{equation*}
            u(t) = K_d \frac{de(t)}{dt}
        \end{equation*}
        \eqlabel{eq:d-regler}{D-Regler}
    \end{figure}


    \subsection{PID-Regler} \label{pid_regler:subsection}
    Ein PID-Regler ist ein häufig verwendeter Reglertyp in der Regelungstechnik. Die Abkürzung PID steht für Proportional-Integral-Derivative, was die drei Hauptkomponenten des Reglers beschreibt. Der proportionale Teil des Reglers reagiert proportional zur Abweichung zwischen der gemessenen Prozessgröße und dem gewünschten Wert, der integrale Teil berücksichtigt die vergangene Abweichung und der derivativen Teil reagiert auf die Geschwindigkeit, mit der sich die Abweichung ändert.

    Eine genauere Beschreibung der einzelnen Regelglieder finden Sie in den Kapiteln: 
    \begin{itemize}
        \item{P-Glied \ref{chp:p-part}}
        \item{I-Glied \ref{chp:i-part}}
        \item{D-Glied \ref{chp:d-part}}
    \end{itemize}


Die Kombination dieser drei Komponenten ermöglicht es dem PID-Regler, schnell auf Veränderungen des Prozesses zu reagieren und gleichzeitig stabile Regelungsergebnisse zu erzielen. Die Einstellung der Reglerparameter, wie z.B. des Proportionalitätsfaktors, des Integrationszeitraums und des Differentiationszeitraums, ist jedoch eine wichtige Herausforderung bei der Anwendung von PID-Reglern.

PID-Regler finden in vielen Anwendungen Anwendung, einschließlich Temperaturregelung, Geschwindigkeitsregelung, Positionierung und Flugzeugsteuerung.
   
\subsection{Flightstack}

Der Flightstack eines Multicopters besteht aus mehreren Systemen. Üblicherweise besteht der Flightstack aus Mikrocontrollern, Sensoren, Aktuatoren, GPS-Modul und einer Stromversorgung. Die Sensoren sind dafür zuständig Daten über die aktuelle Fluglage des Multicopters zu sammeln. Daten die gesammelt werden sind Beschleunigung, Neigung, Lage und GPS-Position. Diese Daten werden vom Mikrocontroller verarbeitet um die Steuerung der Aktuatoren zu optimieren. Im Outdoor Flug ermöglicht die Verwendung eines GPS-Moduls die Position des Multicopters zu bestimmen und zu verfolgen.

Da die verbaute Technik auch mit Strom versorgt werden muss, ist ein Akkumulator auf dem Multicopter verbaut. Der Akkumulator stellt die nötige Energie für den Betrieb des Multicopters bereit. Insgesamt ist der FlightStack ein zentrales Element zur Steuerung des Multicopters und erfordert eine sorgfältige Integration und Abstimmung aller Komponenten, um eine zuverlässige und sichere Steuerung zu gewährleisten.
\subsection{Anforderungen an SLAM Systeme auf Multicoptern}

\begin{itemize}
    \item[Echtzeitfähigkeit]{Da Multicopter sehr schnell auf Änderungen reagieren müssen, muss der SLAM Algorithmus in Echtzeit arbeiten können. Somit wird sichergestellt, dass genaue und aktuelle Informationen zur Verfügung stehen}
    \item[Effizienz]
    \item[Genauigkeit]
    \item[Robustheit]
    \item[Skalierbarkeit]    
\end{itemize}

\ac{SLAM} Systeme auf Multicoptern 

\section{3D-Modelle} \label{3d-modelle:section}
3D-Modelle sind digitale Nachbildungen von physischen Objekten oder Szenen in drei Dimensionen. Diese Modelle werden in verschiedenen Branchen und Anwendungen eingesetzt, von der Architektur und Ingenieurwesen über Film und Gaming bis hin zur Medizin und dem Design von Produkten.

Die Erstellung von 3D-Modellen erfolgt in der Regel durch eine Kombination von verschiedenen Technologien und Software-Tools. Zum Beispiel können 3D-Scanner verwendet werden, um physische Objekte zu scannen und daraus digitale Modelle zu erstellen. Alternativ können 3D-Modelle auch von Grund auf neu erstellt werden, indem man eine 2D-Zeichnung oder ein Konzept in einer 3D-Software modelliert.

Ein wichtiger Vorteil von 3D-Modellen ist, dass sie interaktiv und manipulierbar sind. Das bedeutet, dass man sie drehen, skalieren und animieren kann, um verschiedene Perspektiven oder Bewegungen zu erzeugen. Darüber hinaus können 3D-Modelle auch für Simulationen und Analysen verwendet werden. Zum Beispiel können Architekten und Ingenieure 3D-Modelle von Gebäuden erstellen, um die Belastung bei Erdbeben oder starkem Wind zu simulieren.

Insgesamt sind 3D-Modelle eine äußerst vielseitige Technologie, die in vielen verschiedenen Branchen und Anwendungen eingesetzt werden kann. Sie ermöglichen es, physische Objekte und Szenen digital darzustellen und zu manipulieren, was zahlreiche Möglichkeiten für Design, Simulation und Analyse bietet.

\subsection{3D-Scanner}
Ein 3D-Scanner ist ein Gerät, das Objekte oder Umgebungen erfasst und digitale 3D-Modelle erstellt. Im Gegensatz zu einem herkömmlichen Scanner, der nur flache Bilder erzeugt, kann ein 3D-Scanner eine vollständige dreidimensionale Darstellung eines Objekts erstellen.

Es gibt verschiedene Arten von 3D-Scannern, darunter Laserscanner, strukturiertes Licht, Fotogrammetrie und Time-of-Flight-Scanner. Jeder dieser Scanner verwendet unterschiedliche Technologien und Methoden zur Erfassung von Daten.

Laserscanner verwenden Laserlicht, um die Form eines Objekts zu erfassen, während strukturiertes Licht eine Reihe von Mustern projiziert, um die Oberfläche eines Objekts zu messen. Fotogrammetrie nutzt Bilder, die von verschiedenen Blickwinkeln aufgenommen werden, um ein 3D-Modell zu erstellen, während Time-of-Flight-Scanner die Zeit messen, die benötigt wird, um einen Lichtpuls zu reflektieren, um Entfernungen zu messen.

Die Verwendung von 3D-Scannern ist in vielen Branchen und Anwendungen weit verbreitet, einschließlich Reverse Engineering, Architektur, Medizin, Unterhaltung, Kunst und Design. 3D-Scanner ermöglichen es den Benutzern, genaue Messungen von Objekten zu erhalten, die in der realen Welt existieren, und diese in digitale Formate zu konvertieren, die leichter zu bearbeiten und zu teilen sind.

Es gibt auch mobile 3D-Scanner, die in Handys und Tablets integriert sind, und diese ermöglichen es Benutzern, schnell und einfach 3D-Modelle von Objekten zu erstellen, ohne dass zusätzliche Geräte erforderlich sind.

Obwohl 3D-Scanner ein unglaublich leistungsfähiges Werkzeug sind, haben sie auch einige Einschränkungen. Zum Beispiel können sie Schwierigkeiten haben, sehr dunkle oder glänzende Objekte genau zu erfassen. Trotzdem sind 3D-Scanner ein wertvolles Instrument für jeden, der in der Lage sein möchte, genaue 3D-Modelle von Objekten oder Räumen zu erstellen und sie in digitale Formate zu konvertieren.

\subsubsection{Lasertriangulation}
Bei der Lasertriangulation wird ein Laserstrahl wird an der Oberfläche des Messobjektes reflektiert und über eine Optik und einen Umlenkspiegel auf eine lichtempfindliche Zeilenkamera projiziert. Abhängig von der Entfernung des Messobjektes verändert sich die Position des Lichtpunktes. Hieraus ermittelt der Signalprozessor den Abstand zwischen dem Sensor und der Oberfläche der Stahlprodukte.

\subsubsection{3D-Scanner mit strukturiertem Licht}
Scanner für strukturiertes Licht verwenden ebenfalls die trigonometrische Triangulation. Allerdings verwenden sie keinen Laser. Das System pojeziert eine Reihe an linienförmigen Mustern of auf Objekt. Zum Erkennen des Abtandes werden die Kanten jeder Linie des Musters untersucht. Anhand dieser Kanten kann dann dier Abstand vom Scanner zum Objekt berechnet werden. Im Wesentlichen „sieht“ die Kamera nicht eine Laserlinie, sondern den Rand des projizierten Musters

\subsubsection{Time-of-flight-Scanner}
Time-of-Flight (ToF) Scanner sind eine Art von 3D-Scanner, die mithilfe von Lichtimpulsen die Entfernung zu Objekten in Echtzeit messen können. Sie haben in den letzten Jahren aufgrund ihrer hohen Genauigkeit und ihrer Fähigkeit, schnelle und präzise Messungen durchzuführen, eine breite Anwendung in verschiedenen Branchen gefunden.

ToF-Scanner verwenden in der Regel eine Kombination aus Infrarot-Licht und Kameras, um Tiefeninformationen zu sammeln. Der Scanner sendet einen kurzen Lichtimpuls aus und misst dann die Zeit, die benötigt wird, um zum Objekt zurückzukehren. Durch die Messung der Zeit und die Analyse des reflektierten Lichts kann der Scanner die Entfernung zum Objekt bestimmen und daraus ein 3D-Modell des Objekts erstellen.

\subsubsection{Laser-Phasenverschiebungs-Scanner}
Laser-Phasenverschiebungs-Scanner arbeiten auf eine sehr ähnliche Art wie die Time-of-flight-Scanner. Der Scanner verwendet nicht nur die Zeit, die der Laser braucht, sondern wertet auch seine Phasenverschiebung, welche auf dem Weg vom Senden und Empfangen entsteht aus.

Der Laserstrahl wird auf das Objekt gerichtet und reflektiert zurück zur Kamera. Das reflektierte Licht enthält Informationen über die Form des Objekts, die durch die Phasenverschiebung des Lichts bestimmt werden kann. Die Phasenverschiebung wird gemessen, indem der Laserstrahl in unterschiedlichen Winkeln auf das Objekt gerichtet wird, was zu unterschiedlichen Phasenverschiebungen führt. Die Kamera nimmt Bilder des reflektierten Lichts auf und die Phasenverschiebung der reflektierten Lichtwellen wird berechnet. Diese Informationen werden dann verwendet, um ein 3D-Modell des Objekts zu erstellen.

Der Scanner vergleicht die Phase des Aussendens des Lasers mit der Rückführung des Lasers zum Sensor. Die Messung der Phasenverschiebung ist eine präzisere Methode.

\subsection{Tiefenkammera}
Eine Tiefenkamera ist eine Art von Kamera, die mithilfe von verschiedenen Technologien wie beispielsweise Stereovision oder Time-of-Flight (ToF) die Entfernung von Objekten im Bild ermitteln kann.

Bei der Stereovision werden zwei Kameras verwendet, die jeweils dasselbe Objekt aus einer etwas anderen Perspektive betrachten. Durch die Analyse der Unterschiede in den beiden Bildern kann die Tiefenkamera die Entfernung zu den Objekten im Bild bestimmen.

Die Time-of-Flight-Technologie verwendet dagegen einen Laser oder eine LED, um Lichtimpulse auf die Umgebung zu senden und die Zeit zu messen, die benötigt wird, um vom Objekt zurückzukehren. Je länger das Licht braucht, um zurückzukehren, desto weiter entfernt ist das Objekt. Diese Technologie ermöglicht es der Tiefenkamera, die Entfernung zu Objekten in Echtzeit zu messen und sogar Bewegungen in Echtzeit zu verfolgen.

Die erfassten Tiefendaten können dann verwendet werden, um beispielsweise 3D-Modelle von Objekten oder Umgebungen zu erstellen.

\subsection{Punktwolken}
Eine Punktwolke ist eine räumliche Darstellung eines Objekts oder einer Szene in Form einer Sammlung von 3D-Koordinaten. Am Beispiel von \ac{RGB-D} Kameras, erfasst eine Punktwolke sowohl Farb- als auch Tiefeninformationen aus einer Szene.
Die Tiefeninformationen werden verwendet, um die räumliche Geometrie der Szene zu erfassen und eine Punktwolke zu erstellen.
Die Punktwolke besteht aus einer Sammlung von Punkten, die jeweils eine Position im dreidimensionalen Raum repräsentieren. Jeder Punkt hat in der Regel drei Koordinaten (x, y, z), die seine Position im Raum beschreiben. Die Farbinformationen werden häufig als RGB-Werte (rot, grün, blau) oder als Grauwerte für jeden Punkt in der Punktwolke gespeichert.
Eine \ac{RGB-D}-Kamera erfasst die Tiefeninformationen durch die Verwendung eines Strukturlichtverfahrens oder eines Zeitflugverfahrens. Strukturlichtverfahren arbeiten durch die Projektion von Lichtmuster auf die Szene und das Erfassen der Verzerrungen im Muster, um die Tiefeninformationen zu berechnen. Zeitflugverfahren funktionieren durch die Messung der Zeit, die benötigt wird, um ein Lichtsignal auszusenden und das reflektierte Signal wieder zu empfangen, um die Entfernung und damit die Tiefeninformationen zu berechnen.

Die erfasste Punktwolke wird dann für verschiedene Anwendungen verwendet, wie z.B. für die Erstellung von 3D-Modellen, für die Positionsbestimmung von Robotern oder für die Objekterkennung in der Robotik. Da die Punktwolken alle Informationen über die räumliche Geometrie einer Szene enthalten, sind sie ein wichtiges Instrument für viele Anwendungen in der Robotik, der Computer Vision und der Augmented Reality.


\section{Positionsbestimmung und Kartenerstellung}
Positionsbestimmung ist ein wichtiges Thema in vielen Bereichen, wie der Robotik, autonomem Fahren, Navigation und Augmented Reality. Eine der Herausforderungen bei der Positionsbestimmung ist die Erstellung einer genauen Karte der Umgebung, die es dem mobilen Gerät ermöglicht, seine Position darin zu bestimmen. Hier kommen Technologien wie SLAM (Simultaneous Localization and Mapping) und Spatial Mapping zum Einsatz. Beide Ansätze ermöglichen die Erstellung von 3D-Karten der Umgebung, aber sie unterscheiden sich in ihren Methoden und Anwendungen. In dieser Hinsicht sind SLAM und Spatial Mapping wichtige Technologien für die Positionsbestimmung und haben eine breite Anwendung in verschiedenen Branchen gefunden.

    \subsection{Koordinatensysteme}

\begin{description}
    \item[Welt Koordinatensystem] 
\end{description}
    \todo{Koordinatensysteme}



    \cite[vgl. ]{SWB-1841134112}
    \todo{schreiben?}

 \subsection{Extended Kalman Filter 2}
    Der \ac{EKF}2-Algorithmus, der in der Drohnensteuerung verwendet wird, ist eine Erweiterung des klassischen erweiterten Kalman-Filters, die speziell auf die Bedürfnisse der Drohnensteuerung zugeschnitten ist. Der EKF2 ist ein geschätzter Zustandsregler, der die aktuellen Zustände (z.B. Position, Geschwindigkeit, Orientierung) einer Drohne schätzt, basierend auf Messungen von Sensoren (z.B. \ac{GPS}, \ac{IMU}, Magnetometer).

Im Gegensatz zum klassischen EKF verwendet der EKF2 eine modifizierte Version der Kalman-Filter-Formeln, um den Einfluss von Sensorrauschen und Messfehlern besser zu berücksichtigen. Insbesondere verwendet der EKF2 eine sogenannte "Innovation Covariance Matrix", die die Varianz der Messfehler repräsentiert und in die Filtergleichungen eingebaut wird. Diese Innovation Covariance Matrix wird iterativ während des Betriebs des Filters aktualisiert, um den Sensorrauschen und Messfehlern besser gerecht zu werden.

Darüber hinaus verwendet der EKF2 eine modifizierte Version der State Transition Matrix, die die Nichtlinearitäten des Systems besser modellieren kann. Diese modifizierte Matrix wird ebenfalls iterativ während des Filterbetriebs aktualisiert, um den Änderungen im Systemverhalten besser gerecht zu werden.

In der Drohnensteuerung wird der EKF2-Algorithmus verwendet, um die aktuellen Zustände der Drohne (z.B. Position, Geschwindigkeit, Orientierung) in Echtzeit zu schätzen. Diese Schätzungen werden dann verwendet, um die Steuerbefehle der Drohne zu generieren, um sie auf Kurs zu halten und sicher zu navigieren.z
    
    \subsection{Spatial Mapping} \label{spatial_mapping:subsection}
    Spatial Mapping hingegen ist ein Prozess, bei dem eine 3D-Karte der Umgebung auf der Grundlage der Verarbeitung von Kameradaten erstellt wird. Im Gegensatz zum SLAM-Prozess muss das mobile Gerät, das die 3D-Karte erstellt, seine Position in der Umgebung bereits kennen, da es auf der Verarbeitung von Kameradaten basiert. Die Kameras nehmen Bilder auf und wandeln sie in 3D-Punktwolken um, die dann zu einer Karte kombiniert werden. Spatial Mapping wird in Augmented-Reality-Anwendungen und anderen Anwendungen eingesetzt, bei denen eine genaue 3D-Karte der Umgebung benötigt wird.
    Spatial Mapping hat jedoch einige Einschränkungen.
    Es ist schwierig eine genaue Karte zu erstellen, wenn sich das mobile Gerät schnell bewegt oder die Umgebung stark verändert. Außerdem ist die Anwendungsbreite von Spatial Mapping auf Anwendungen beschränkt, bei denen eine genaue Karte ausreichend ist, ohne das eine kontinuierliche Positionsschätzung erforderlich ist.
   
    \subsection{Inertielle Positionsbestimmung} \label{inertielle_positionsbestimmung:subsection}
    Inertiale Positionsbestimmung ist ein weiterer Ansatz zur Positionsbestimmung, der auf der Verwendung von Inertialsensoren wie Gyroskopen und Beschleunigungsmessern basiert. Inertialsensoren messen die Veränderungen der Beschleunigung und der Winkelgeschwindigkeit eines mobilen Geräts, was dazu verwendet werden kann, die Position und Orientierung des Geräts in der Umgebung zu bestimmen. Die Inertiale Positionsbestimmung hat jedoch auch einige Herausforderungen, da die Inertialsensoren anfällig für Driftfehler sind. Diese Fehler können durch die Integration der Beschleunigung und der Winkelgeschwindigkeit über die Zeit entstehen und zu Ungenauigkeiten bei der Positionsbestimmung führen. Um diesen Fehler zu korrigieren können Inertialsensoren mit anderen Sensoren wie Magnetometern oder GPS kombiniert werden.
    Eine andere Möglichkeit besteht darin, die Inertialsensor-DAten in einen SLAM- oder Spatial Mapping-Prozess zu integrieren, um eine genauere Positionsbestimmung zu erzielen. Durch die Kombination von Inertialsensor-DAten mit visuellen DAten können die Vorteile beider Ansätze genutzt werden und die Nachteile minimiert werden.


Allerdings ist die inertielle Positionsbestimmung anfällig für Fehler, die aufgrund von Drift und Ungenauigkeiten der Inertialsensoren entstehen können. Um diese Fehler zu minimieren, wird die inertielle Positionsbestimmung oft mit anderen Technologien wie GPS oder visuellen Systemen kombiniert. Diese Kombination von Technologien wird auch als Sensorfusion bezeichnet und ermöglicht eine präzisere und robustere Positionsbestimmung in verschiedenen Anwendungen wie Drohnen, autonomen Fahrzeugen und Wearables.

\subsection{SLAM}\label{SLAM:section} \label{positionsbestimmung:section}

Der SLAM-Prozess umfasst die simultane Bestimmung der Position eines mobilen Geräts in der Umgebung und die Erstellung einer Karte dieser Umgebung. Dies wird durch die Integration von Daten aus verschiedenen Sensoren wie Kameras, Inertialsensoren und Entfernungsmessern wie Laser-Scannern oder Time-of-Flight-Sensoren erreicht. Der SLAM-Prozess wird oft in der Robotik und in autonomen Fahrzeugen eingesetzt, um eine präzise Karte der Umgebung zu erstellen und sich gleichzeitig darin zu lokalisieren.
Durch die Kombination der Inertialsensoren mit anderen Sensoren kann eine präzisere und robustere Positionsbestimmung erreicht werden, indem die Vorteile der verschiedenen Technologien kombiniert werden. Die Inertialsensoren können beispielsweise verwendet werden, um die Bewegung des mobilen Geräts zwischen den einzelnen Beobachtungen durch die anderen Sensoren zu messen und die Positions- und Orientierungsdaten der SLAM-Systeme zu korrigieren.

Ein SLAM System besteht aus verschiedenen Teilaspekten.

\subsubsection{SLAM Grundlagen}

\subsubsection{Datenaquirierung}



\subsubsection{Punktwolken}

Punktwolken sind ein zentraler Bestandteil von 3D-Modellen und werden in verschiedenen Anwendungen wie 3D-Rekonstruktion, Objekterkennung und Augmented Reality eingesetzt. Sie stellen eine dreidimensionale Repräsentation der realen Welt dar und bestehen aus einer großen Anzahl von Punkten, die die räumliche Struktur und Form der Objekte in der Szene beschreiben.

Ein Punktwolkengenerator erzeugt eine Punktwolke aus mehreren Bildern, die von einer Kamera aufgenommen wurden.

Um die Punktwolke zu erzeugen müssen 2D Koordinaten in ein 3D Koordinatensystem überführt werden, dazu benötigt man zunächst die intrinsische Matrix der Kamera. Diese Matrix beschreibt die Abbildungseigenschaften der Kamera und enthält Informationen wie Brennweite, Verschiebung und Verzerrung. Um diese Matrix zu erhalten, werden Kalibrierungsbilder von der Kamera aufgenommen und anschließend eine Kalibrierungssoftware verwendet, um die intrinsischen Parameter der Kamera zu berechnen. Die intrinsischen Parameter

Sobald die intrinsische Matrix bekannt ist, kann die extrinsische Matrix bestimmt werden. Diese Matrix beschreibt die Position und Ausrichtung der Kamera im Raum relativ zum Weltkoordinatensystem. Um diese Matrix zu berechnen, werden mehrere Bilder von der Kamera aus verschiedenen Positionen aufgenommen und mithilfe von visuellen Odometrie- oder SLAM-Algorithmen verarbeitet, um die Kameraposition und -orientierung zu schätzen.

Die extrinsische Matrix ist eine 4x4-Matrix, die in homogenen Koordinaten ausgedrückt wird und die Transformation von Kamerakoordinaten in Weltkoordinaten beschreibt. Diese Transformation umfasst die Translation der Kamera und die Rotation um die Achsen des Weltkoordinatensystems. Die extrinsische Matrix wird verwendet, um die 3D-Position jedes Punktes in der Szene relativ zur Kamera zu bestimmen.

In Kombination mit der intrinsischen Matrix ermöglicht die extrinsische Matrix die Umwandlung von 2D-Bildpunkten in korrekte 3D-Objektpunkte. Die intrinsischen Parameter beschreiben die Abbildungseigenschaften der Kamera und die extrinsischen Parameter beschreiben die Position und Orientierung der Kamera relativ zum Weltkoordinatensystem. Zusammen ermöglichen sie die genaue Rückprojektion von 2D-Bildpunkten in den 3D-Raum und somit die Erzeugung einer genauen Punktwolke. 

Die Transformationsmatrix, die die Transformation von Kamerakoordinaten in Weltkoordinaten beschreibt, kann aus der intrinsischen Matrix und der extrinsischen Matrix berechnet werden. Die Formel finden Sie in \ref{eq:transform-matrix}

\cite{SWB-470983582}

\begin{figure}
\begin{equation*}
T = K \begin{bmatrix}R & t\end{bmatrix}
\end{equation*}
\eqlabel{eq:transform-matrix}{Transformationsmatix}
wobei $K$ die intrinsische Matrix, $R$ die 3x3 Rotationsmatrix aus der extrinsischen Matrix und $t$ der 3x1 Translationsvektor aus der extrinsischen Matrix ist. Das Symbol $\begin{bmatrix}R & t\end{bmatrix}$ bezeichnet die erweiterte Matrix, die aus der Kombination von $R$ und $t$ gebildet wird.
\end{figure}

Durch Wiederholung dieses Prozesses für jedes 2D-Bild wird eine Punktwolke erzeugt, die eine dreidimensionale Repräsentation der Szene darstellt. Ein wichtiger Aspekt bei der Erstellung einer Punktwolke ist es, dass die Bilder aus verschiedenen Blickwinkeln aufgenommen werden. Dadurch kann eine größere Menge an Information über die Szene erfasst werden, was zu einer genaueren und detaillierteren Punktwolke führt.

Es ist auch zu beachten, dass bei der Erzeugung von Punktwolken aus \ac{RGB-D}-Bildern eine weitere wichtige Matrix verwendet wird: die Transformationsmatrix. Diese Matrix beschreibt die Beziehung zwischen der Kamera und dem Tiefensensor und ermöglicht es, die 3D-Position jedes Punktes in der Szene basierend auf der Tiefeninformation des Sensors zu bestimmen.

Insgesamt ist die Erzeugung einer Punktwolke ein komplexer Prozess, der sowohl die intrinsischen als auch extrinsischen Parameter der Kamera erfordert. Es erfordert auch die Verarbeitung von mehreren Bildern aus verschiedenen Blickwinkeln und gegebenenfalls die Verwendung von Tiefensensoren. Trotzdem ist die Erzeugung von Punktwolken ein wichtiger Schritt für viele Anwendungen in der Computergrafik und -vision. Zum Beispiel kann die Punktwolke für die Erstellung von 3D-Modellen verwendet werden, um virtuelle Umgebungen für Computerspiele und Simulationen zu erstellen oder für die Erkennung von Objekten und Hindernissen in autonomen Fahrzeugen.

Es gibt viele Algorithmen und Technologien, die für die Erzeugung von Punktwolken verwendet werden können, darunter Stereo-Vision, Structured-Light-Scanning und Time-of-Flight-Sensoren. Jede Methode hat ihre eigenen Vor- und Nachteile, und die Wahl der am besten geeigneten Methode hängt von den Anforderungen der Anwendung ab.



\begin{figure}
\begin{equation*}
    K = \begin{bmatrix}
    f_x & s & x_0 \\
    0 & f_y & y_0 \\
    0 & 0 & 1
    \end{bmatrix}
    \end{equation*}
    \caption{Intrinsische Matrix für eine \ac{RGBD} camera}
    %\label{eq :intrinsic-matrix}
    \eqlabel{eq:intrinsic-matrix}{Intrinsische Maxtrix}
\end{figure}

\begin{figure}
    \begin{equation*}
        T_{ext}=\begin{bmatrix}
            r_{11} & r_{12} & r_{13} & t_x\\
            r_{21} & r_{22} & r_{23} & t_y\\
            r_{31} & r_{32} & r_{33} & t_z\\
            0 & 0 & 0 & 1
            \end{bmatrix}   
    \end{equation*}
    \eqlabel{eq:extrinsic-matrix}{Extrinsische Matrix}
\end{figure}

\subsubsection{Odometrie und Bewegunsschätzung}

Die Odometrie ist ein Verfahren zur Bestimmung der Position und Bewegung eines Roboters auf der Grundlage der Messung von Raddrehungen. Da Multicopter keine Räder zur Fortbewegung einsetzen müssen andere Arten von Odometrie eingesetzt werden. Bei Multicoptern können drei Arten von Odometrie eingesetzt werden: die visuelle Odometrie, die inerzielle Odometrie und die GPS-basierte Odometrie.

\todo{Odometrie mit Raddrehungen anpassen für Multicopter}

Bei der visuellen Odometrie werden Kameras verwendet, um die Bewegung des Multicopters zu verfolgen. Die Kamera erfasst Bilder der Umgebung und berechnet anhand von Merkmalen in den Bildern, wie sich der Multicopter bewegt. Die inertielle Odometrie nutzt Beschleunigungsmesser und Drehratenmesser, um die Beschleunigung und Drehung des Multicopters zu messen. Die GPS-basierte Odometrie verwendet Satellitensignale, um die Position und Bewegung des Multicopters zu bestimmen.

SLAM (Simultaneous Localization and Mapping) kann für die Verwendung von Odometrie bei Multicoptern eingesetzt werden. SLAM ist ein Verfahren, bei dem ein Roboter seine Position in Echtzeit bestimmt und gleichzeitig eine Karte der Umgebung erstellt. Bei Verwendung von SLAM mit der Odometrie können Fehler bei der Odometrie-Kartierung durch Korrektur der Odometrie-Schätzungen mit anderen Sensorinformationen, wie GPS oder visuelle Daten, korrigiert werden.

Zusammenfassend ermöglicht die Odometrie bei Multicoptern eine präzise Bestimmung der Position und Bewegung des Roboters, insbesondere in Umgebungen, in denen GPS-Signale nicht verfügbar oder unzuverlässig sind. Die Verwendung von SLAM mit Odometrie ermöglicht es, die Genauigkeit der Lokalisierung zu verbessern und eine Karte der Umgebung zu erstellen.

\subsection{SLAM-Verfahren}

\subsubsection{OBD-SLAM3}

BD-SLAM3 ist eine Erweiterung des OBD-SLAM (Oriented Bounding Box SLAM) Algorithmus, der für die simultane Lokalisierung und Kartierung (SLAM) in dreidimensionalen Umgebungen entwickelt wurde. Im Gegensatz zu OBD-SLAM verwendet OBD-SLAM3 jedoch eine tiefere Netzwerkarchitektur, um die Leistung und Genauigkeit der Odometrie und der visuellen Wahrnehmung zu verbessern.

OBD-SLAM3 nutzt eine Kombination aus Tiefenkameras und RGB-Kameras, um eine 3D-Repräsentation der Umgebung zu erstellen und gleichzeitig die Bewegung des Roboters in der Umgebung zu schätzen. Der Algorithmus verwendet ein neuronales Netzwerk, um die Tiefeninformationen der Kameras zu verarbeiten und eine Schätzung der Kamerapositionen und -orientierungen zu generieren.

Im Gegensatz zu anderen SLAM-Verfahren, die auf einer Vielzahl von Features wie Punkten oder Linien basieren, nutzt OBD-SLAM3 Oriented Bounding Boxen (OBBs), um eine robustere Schätzung der Kamerabewegungen und -positionen zu ermöglichen. Die OBBs erlauben eine genauere Modellierung der Umgebung und eine bessere Korrektur von Fehlern, die durch Ungenauigkeiten bei der Berechnung von 3D-Points entstehen können.

Insgesamt ermöglicht OBD-SLAM3 eine genauere und robustere Lokalisierung und Kartierung in dreidimensionalen Umgebungen, was ihn zu einem vielversprechenden Ansatz für autonome Robotikanwendungen macht.

\subsubsection{LSD-SLAM}

LSD-SLAM (Large-Scale Direct Monocular SLAM) ist ein Algorithmus zur simultanen Lokalisierung und Kartierung (SLAM) in Echtzeit, der eine einzelne Kamera verwendet. Im Gegensatz zu anderen SLAM-Methoden, die Merkmale oder Punktwolken verwenden, um eine Karte der Umgebung zu erstellen, nutzt LSD-SLAM eine direkt auf den Bildern basierende Methode. Hierbei werden die Intensitäts- und Gradienteninformationen der Bilder genutzt, um die Kameraposition und -orientierung zu schätzen und eine dichte Punktwolke der Umgebung zu erstellen.

LSD-SLAM wurde von Jakob Engel, Thomas Schöps und Daniel Cremers an der Technischen Universität München entwickelt und im Jahr 2014 vorgestellt. Es hat sich als eine effektive Methode zur SLAM-Problemstellung erwiesen, insbesondere in Umgebungen mit geringer Textur oder schlechter Beleuchtung.



\section{Simulationstechnik} \label{simulationstechnik:section}
Der \ac{VDI} hat in seiner \ac{VDI}-Richtline 3633 Simulation wie folgt definiert: "Nachbilden eines dynamischen Prozesses in einem System mithilfe eines experimentierfähigen Modells, um zu Erkenntnisen zu gelangen, die auf die Wirklichkeit übertragbar sind." \cite{VDI-3633}

\todo{Angegebene Quelle nur Vorläufige Quelle. VDI-3633 ist eigendliche Quelle}

Die durch eine Simulation erlangten Erkenntnisse können unter anderem folgende sein:
\begin{description}
    \item[Verhaltensprognosen] Durch die Simulation können viele verschiedene Verhaltensweisen eines Systems vorhergesagt werden, welche durch verschiedene Situationen und Begebenheiten ausgelöst werden.
    
    \item[Leistungsverbesserungen] Simulationen können helfen, die Leistung von Systemen oder Prozessen zu verbessern. Durch das Verändern von verschiedenen Parametern und Variablen in der Simulation kann die Leistungsveränderung der Systeme beobachten, ohne dabei die Leistung der realen Systeme zu verändern. Anhand der dadurch gewonnen Erkenntnisse kann man dann die Leistung der realen Syteme verbessern.
    
    \item[Risikoanalyse] Simulationen können dazu beitragen, potenzielle Risiken in einem System oder Prozess zu identifizieren und zu bewerten, bevor sie tatsächlich auftreten. Durch Anpassen des Systems oder des Prozesses kann man dann das Auftreten bestimmte Risiken verhindern. Zudem ist es einfacher, falls einer der zufor festgestelletn Risiken auftritt rechtzeitig un mit den richtigen Mitteln zu reagieren, um somit die entstehenden Schäden zu minimieren.
    
    \item[Kostenreduzierung] Durch die Verwendung von Simulationen können Unternehmen Kosten sparen, indem sie durch die Verwendung der Punkte Verhaltensprognosen, Leistungsverbesserungen und Risikoanalyse mögliche Probleme oder ineffiziente Prozesse im Vorfeld erkennen und beheben.
    
    \item[Entscheidungsunterstützung] Simulationen können auch bei der Entscheidungsfindung (gerade in besonders schwierigen Fällen) eine große Hilfe sein. Hierfür werden viele verschiedene Scenarien simuliert und miteinander verglichen. Die dadurch gewonnen Daten können verwendet werden, um die bestmögliche Entscheidung zu treffen.
\end{description}


Simulationstechnik wird verwendet, um die Leistung von Produkten, Maschinen oder Systemen in einer virtuellen Umgebung zu testen und zu optimieren, bevor sie in der realen Welt eingesetzt werden. Mithilfe von Simulationstechnik können Ingenieure und Designer schnell und kosteneffizient verschiedene Designs, Prototypen oder Konfigurationen testen, bevor sie sich auf eine endgültige Lösung festlegen.

\subsection{Simulationsarten}

\subsection{Simulationssoftware}
Simulationssoftware ist eine Art von Software, die entwickelt wurde, um das Verhalten eines Systems oder Prozesses in einer virtuellen Umgebung zu simulieren. Mit Simulationssoftware können Benutzer verschiedene Szenarien und Bedingungen testen, ohne physische Ressourcen oder teure Experimente einzusetzen.

\subsubsection{Gazebo}
Gazebo ist ein leistungsfähiger 3D-Simulator, der speziell für die Simulation von Robotern in komplexen Innen- und Außenumgebungen entwickelt wurde. Im Gegensatz zu herkömmlichen Spiele-Engines bietet Gazebo eine physikalische Simulation mit einem hohen Grad an Genauigkeit, die es Entwicklern ermöglicht, die Leistung von Robotern in realistischen Umgebungen zu testen.

Eine der herausragenden Eigenschaften von Gazebo ist seine Fähigkeit, Roboterpopulationen in komplexen Umgebungen zu simulieren. Die Software ermöglicht es Entwicklern, mehrere Roboter gleichzeitig zu simulieren und ihr Verhalten in einer Vielzahl von Szenarien zu testen. Dies ist besonders nützlich für die Entwicklung von autonomen Robotersystemen, bei denen die Interaktion und Zusammenarbeit zwischen mehreren Robotern in einer Umgebung von entscheidender Bedeutung ist.

Gazebo bietet auch eine Vielzahl von Sensoren und Schnittstellen für Benutzer und Programme. Dazu gehören Kamera-, Lidar- und GPS-Sensoren sowie ROS-Interfaces für die Integration mit anderen Robotersoftware-Tools. Diese Sensoren und Schnittstellen ermöglichen es Entwicklern, die Leistung von Robotern in verschiedenen Situationen zu testen und zu optimieren.
\cite[vgl.][]{gazebosim}

\subsection{SIL - Software-in-the-loop}

\ac{SIL} ist eine Technik zur Überprüfung und Validierung von Code in einer Simulationsumgebung, um effektiv und kosteneffizient Fehler zu finden und die Code-Qualität zu verbessern. Die SIL-Tests werden typischerweise in den frühen Phasen des Software-Entwicklungsprozesses durchgeführt, um sicherzustellen, dass der Code den Anforderungen entspricht und fehlerfrei ist. Diese Methode kann dazu beitragen, Zeit und Geld zu sparen, da Fehler frühzeitig erkannt und behoben werden können, bevor teure Hardware-Tests durchgeführt werden müssen.

Im Gegensatz zum \ac{HIL}-Ansatz, bei dem die Software auf echten Hardwarekomponenten getestet wird, verwendet \ac{SIL} Simulationen, um die Hardware-Plattformen zu ersetzen. Dies ermöglicht es Entwicklern, die Software auf unterschiedlichen Szenarien und Bedingungen zu testen, ohne tatsächliche Hardwarekomponenten zu benötigen. Darüber hinaus können Entwickler durch \ac{SIL}-Tests den Code in einer Vielzahl von Szenarien testen und Fehler beheben, bevor er in den \ac{HIL}-Test übergeht.


\section{Problembehebung} \label{problembehebung:section}
